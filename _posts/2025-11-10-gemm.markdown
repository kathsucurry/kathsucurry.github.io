---
layout: post
title:  "GEMM"
date:   2025-11-10
categories: cuda
katex: true
---

It's widely known that implementing and optimizing GEMM (**GE**neral **M**atrix **M**ultiplication) is fundamental when it comes to learning GPU programming. In this post, we'll walk through the process of implementing and optimizing single-precision and half-precision GEMM: from a brief overview of what the operation does to advanced optimizations using Tensor Cores.

I'll primarily use [Simon's](https://siboehm.com/articles/22/CUDA-MMM) and [Alex's](https://alexarmbr.github.io/2024/08/10/How-To-Write-A-Fast-Matrix-Multiplication-From-Scratch-With-Tensor-Cores.html#how-to-use-tensor-cores) GEMM posts as the foundation. Specifically, I'll start with reimplementing the kernels from both posts and documenting the insights I gain along the way. My goal is not to replace any of their posts, but to solidify my own understanding of the different optimization techniques and to write a reference I can return to when needed in the future. 

All performance measurements in this posts were taken on an RTX 5070 Ti and the implementation code can be found [here](https://github.com/kathsucurry/cuda_matrix_multiplication).

This post is organized into the following sections:

1. [**What does GEMM do?**](#1-what-does-gemm-do)
2. [**Compute-bound or memory-bound? Introducing the roofline model**](#2-compute-bound-or-memory-bound-introducing-the-roofline-model)
3. [**Problem setup**](#3-problem-setup)
4. [**Single-precision GEMM implementations**](#4-single-precision-gemm-implementations)
    - [**Kernel 01: Naive implementation**](#kernel-01-naive-implementation)
    - [**Kernel 02: Block tiling**](#kernel-02-block-tiling)
    - [**Kernel 03: 2D thread coarsening**](#kernel-03-2d-thread-coarsening)
    - [**Kernel 04: Vectorized memory access**](#kernel-04-vectorized-memory-access)
    - [**Kernel 05: Warp tiling**](#kernel-05-warp-tiling)
    - [**Kernel 06: Warp tiling, subdivided**](#kernel-06-warp-tiling-subdivided)
    - [**Kernel 07: Transpose `As`**](#kernel-07-transpose-as)
    - [**Kernel 08: Asynchronous global-memory loads + double buffering**](#kernel-08-asynchronous-global-memory-loads--double-buffering)
5. [**Tensor cores**](#5-tensor-cores)
6. [**Half-precision GEMM implementation**](#6-half-precision-gemm-implementations)
    - [**Kernel 09: Tensor cores (wmma API)**](#kernel-09-tensor-cores-wmma-api)
    - [**Kernel 10 & 11: Tensor cores + asynchronous gmem loads + double buffering**](#kernel-10--11-tensor-cores--asynchronous-gmem-loads--double-buffering)
    - [**Kernel 12: Tensor cores + three-level pipeline**](#kernel-12-tensor-cores--three-level-pipeline)
    - [**Kernel 13: Tensor cores (mma instructions)**](#kernel-13-tensor-cores-mma-instructions)
6. **Summary**

<hr class="hr-top" /><hr />

# 1. What does GEMM do?

Given three matrices $A$, $B$, and $C$, and two scalar values $\alpha$ and $\beta$, GEMM performs the following operation:

$$
C = \alpha A B + \beta C
$$

A standard matrix multiplication $AB$ is a case of GEMM where $\alpha = 1$ and $\beta = 0$.

The matrix multiplication $AB$ by itself can be illustrated by the figure below.

![image Matrix Multiplication](/assets/images/2025-11-10-gemm/intro_matmul.png)
<p style="text-align: center;"><i>Matrix multiplication between $A$ (dimension: $M \times K$) and $B$ (dimension: $K \times N$), resulting in a matrix with dimension ($M \times N$).</i></p>

To compute the value of `AB[y, x]`, with $y$ indicating the matrix row and $x$ indicating the matrix column, we perform a dot product between the entire row $y$ of $A$ and the entire column $x$ of $B$:

$$
AB[y, x] = \sum_{k=1}^{K} A[y, k] * B[k, x]
$$

<hr class="hr-top" /><hr />

# 2. Compute-bound or memory-bound? Introducing the roofline model

In my earlier [Reduction (Sum) post]({% link _posts/2025-10-14-reduction_sum_part1.markdown %}), I mentioned that reduction is an example of a **memory-bound** kernel. But what about GEMM? How can we determine whether it's **compute-bound** or **memory-bound** in the first place? To answer these questions, we'd need to talk about **the Roofline Model**. 

The Roofline Model is designed to guide the optimization process by defining the *peak performance* achievable on a given hardware. It's typically visualized as a 2D plot, with **Operational Intensity** on the $x$-axis and **Achievable Throughput** on the $y$-axis.

![image Roofline model](/assets/images/2025-11-10-gemm/intro_roofline.png)
<p style="text-align: center;"><i>The roofline model.</i></p>

**Operational Intensity** measures the number of floating-point operations performed per byte of memory transferred (FLOPs/Bytes).

On the other hand, **Throughput**, or performance, represents the number of floating-point operations a processor can execute per second (FLOPs/s).

The Roofline Model visualizes two types of "roofs" in a 2D plot:

1. **Memory bandwidth roof**, represents the (theoretical) peak memory bandwidth. As noted in the sum reduction post, the RTX 5070 Ti has a peak memory bandwidth of approximately 896 GB/s. This roof appears as a slanted line on the plot, following the equation $Performance = Memory\,Bandwidth \times Operational \, Intensity$.

2. **Compute roof**, represents the theoretical peak throughput of the hardware. Based on the [specification document](https://images.nvidia.com/aem-dam/Solutions/geforce/blackwell/nvidia-rtx-blackwell-gpu-architecture.pdf), RTX5070Ti achieves a peak FP32 throughput of 43.9TFLOPs/s for standard operations and 87.9TFLOPs/s for tensor operations.

Regions below the memory bandwidth roof are considered **memory-bound**, while regions below the compute roof are considered **compute-bound**.

<!-- START OF DIV -->
<div class="div-example">

<h4>Example: determining whether the naive implementation is memory- or compute-bound</h4>

<b>Computing the total floating-point operations</b>

<br /><br />

To compute the total floating-point operations, we can divide the GEMM operations into 4 steps:

<br />

<ol>
<li>
$AB$
<br />
For each cell in $C$ (a total of $M \times N$ number of cells), we compute a dot product of $K$ elements from $A$ and $K$ elements from $B$. So we have a total of $(M \times N) \times (K + K - 1)$ FLOPs, where the first $K$ indicates the number of multiplication operations and the latter $K - 1$ indicates the number of addition operations. Note that some references assume the use of <i>fused multiply-add (FMA)</i> instruction, which consists of 1 FLOP of multiply and 1 FLOP of add, simplifying the number of FLOPs to $2 \times M \times N \times K$.
</li>

<li>
$\alpha AB$
<br />
Once we have $AB$ from step 1, we simply run another $MN$ multiplication operations to obtain $\alpha AB$.
</li>

<li>
$\beta C$
<br />
Similar to step 2, we have another $MN$ FLOPs.
</li>

<li>
$\alpha AB + \beta C$
<br />
In our final addition, we perform another $MN$ FLOPs.
</li>
</ol>

To summarize, we have in total of

$$
(M \times N) \times (K + K - 1) + 3MN = (2MNK + 2MN) \,FLOPs
$$

<b>Computing the total data transferred</b>

<br /><br />

Similarly, we can divide the total data transferred into three steps:

<br/>

<ol>
<li>
$\alpha AB$
<br />
For each cell in $C$, we transfer $K$ elements from $A$ and $K$ elements from $B$, so we have in total of $2MNK \times 4\,Bytes$ data transferred.
</li>

<li>
$\beta C$
<br />
We load all elements of $C$ = $MN \times 4\,Bytes$.
</li>

<li>
$C = \alpha AB + \beta C$ (storing the updated $C$)
<br />
We store all elements of $C$ = $MN \times 4\,Bytes$.
</li>
</ol>

Summing the Bytes transferred in all steps, we'll have 

$$
(2MNK + 2MN) \times 4\,Bytes
$$

<b>Computing the operational intensity</b>

<br /><br />

Given the total FLOPS and total data transfer calculated above, the operational intensity is only <b>0.25 FLOPS/Byte</b>. Even if the effective memory bandwidth approaches the peak bandwidth of 896 GB/s on the RTX5070Ti, this corresponds to a theoretical throughput of just <b>0.224 TFLOPs/s</b>. This value is significantly lower than the GPU's peak non-tensor FP32 performance of <b>43.9 TFLOPs/s</b>, placing the computation within the <b>memory-bound region</b> of the Roofline Model.

<br /><br />

It's important to note that this operational intensity is <i>theoretical</i>. Since elements in $A$ and $B$ are reused across threads and iterations, GPU caches (L1 and L2) automatically serve most repeated accesses; only the <i>first</i> access to a cache line actually goes to DRAM. As a result, Nsight Compute reports a much higher effective operational intensity of <b>18.33 FLOPs/Byte</b>, because caching hides the majority of DRAM traffic.

<br /><br />

Regardless of this difference, the conclusion remains: <b>naive GEMM implementation is memory-bound</b>, and we'll show how optimizing it with different techniques, along with larger $M$, $N$, and $K$ values, shift the performance toward the compute-bound region.

</div>
<!-- END OF DIV -->

<hr class="hr-top" /><hr />

# 3. Problem setup

We aim to optimize the GEMM operation on matrices $A$, $B$, and $C$ with $M = N = K = 4096$. All matrices are stored in **row-major** order.

The performance of each kernel is evaluated relative to **cuBLAS**, which serves as the reference implementation.

<hr class="hr-top" /><hr />

# 4. Single-precision GEMM implementations

In this post, we will look into the following implementations:

1. [**Kernel 01: Naive implementation**](#kernel-01-naive-implementation)
2. [**Kernel 02: Block tiling**](#kernel-02-block-tiling)
3. [**Kernel 03: 2D thread coarsening**](#kernel-03-2d-thread-coarsening)
4. [**Kernel 04: Vectorized memory access**](#kernel-04-vectorized-memory-access)
5. [**Kernel 05: Warp tiling**](#kernel-05-warp-tiling)
6. [**Kernel 06: Warp tiling, subdivided**](#kernel-06-warp-tiling-subdivided)
7. [**Kernel 07: Transpose `As`**](#kernel-07-transpose-as)
8. [**Kernel 08: Asynchronous global-memory loads + double buffering**](#kernel-08-asynchronous-global-memory-loads--double-buffering)

<!-- START OF DIV -->
<div class="div-warning">
<h4>üö® CAUTION üö®</h4>

Before invoking any kernel, we make sure that $M$ and $N$ are both divisible by the block size, so we don't do any boundary check. In practice, however, 
<b>boundary check is necessary to prevent incorrect or undefined behavior</b>.
</div>
<!-- END OF DIV -->

<hr />

#### Kernel 01: Naive implementation

![image Kernel 01: Naive](/assets/images/2025-11-10-gemm/kernel1.png)
<p style="text-align: center;"><i>Kernel 01.</i></p>

In the naive implementation, each `BLOCK_DIM x BLOCK_DIM`-thread block is responsible for computing `BLOCK_DIM x BLOCK_DIM` submatrix of $C$. Within each block, every thread loads $K$ elements from $A$ and $K$ elements from $B$, then performs a dot product over these elements. The corresponding kernel code is shown below.


```c++
template <uint const BLOCK_DIM>
__global__ void __launch_bounds__(BLOCK_DIM * BLOCK_DIM) naive_gemm(
    int M, int N, int K,
    float alpha,
    float const *__restrict__ A,
    float const *__restrict__ B,
    float beta,
    float *__restrict__ *C
) {
    uint const global_x_idx{blockIdx.x * BLOCK_DIM + threadIdx.x};
    uint const global_y_idx{blockIdx.y * BLOCK_DIM + threadIdx.y};

    // Perform boundary check.
    if (global_x_idx < N && global_y_idx < M) {
        float sum{0.0f};
    
        for (uint k{0}; k < K; ++k) {
            sum += A[global_y_idx * K + k] * B[k * N + global_x_idx];
        }
    
        uint const global_idx{global_y_idx * N + global_x_idx};
        C[global_idx] = alpha * sum + beta * C[global_idx];
    }
}
```

<!-- START OF DIV -->
<div class="div-interested">
<h4>If You're Curious...</h4>
The usages of <code>__launch_bounds__()</code> and <code>__restrict__</code> are ways to aid optimization by providing additional information to the compiler. More information on <code>__launch_bounds__()</code> and <code>__restrict__</code> can be found in the CUDA C++ Programming Guide, particularly in sections <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/#launch-bounds">10.38. Launch Bounds</a> and <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/#restrict">10.2.6. __restrict__</a>, respectively.
</div>
<!-- END OF DIV -->

<!-- START OF DIV -->
<div class="div-author-note">
<h4>Author's Note</h4>
If you look at the code, you'd notice that some basic kernel functions have been transformed into function templates so that it can be used for both single- and half-precision GEMMs. You can find the list of implemented kernels <a href="https://github.com/kathsucurry/cuda_matrix_multiplication/blob/main/src/kernels.cuh">here</a>.
</div>
<!-- END OF DIV -->


Note that we use `x` to represent the horizontal axis and `y` to represent the vertical axis. This convention helps achieve better memory coalescing behavior:

- Threads sharing the same `threadIdx.y` but having adjacent `threadIdx.x` values load the same row from $A$. This results in a **broadcast** rather than true memory coalescing.
- Threads with the same `threadIdx.y` and adjacent `threadIdx.x` values load *adjacent elements* from $B$. Because $B$ is stored in row-major order, these accesses are **coalesced**.

The kernel code can then be invoked using the following host code:

```c++
{
    constexpr uint BLOCK_DIM{32};
    assert((N % BLOCK_DIM == 0) && (M % BLOCK_DIM == 0));
    
    dim3 grid_dim(CEIL_DIV(N, BLOCK_DIM), CEIL_DIV(M, BLOCK_DIM));
    dim3 block_dim(BLOCK_DIM, BLOCK_DIM, 1);

    naive_gemm<BLOCK_DIM><<<grid_dim, block_dim>>>(M, N, K, alpha, A, B, beta, C);
}
```

which yields the following performance:

| Kernel # | Performance | % cuBLAS performance |
|:---|:---:|:---:|:---:|
| cuBLAS | 31.66 TFLOPs/s | 100% |
| kernel 01: naive üÜï | 2.05 TFLOPs/s | 6.47% |

<hr />

#### Kernel 02: Block tiling

Recall from Kernel 01 that threads with the same `threadIdx.y` but different `threadIdx.x` values in each block load the **same** row from matrix $A$. Consequently, each element of $A$ is loaded $N$ times, and similarly, each element of $B$ is loaded $M$ times.

One way to reduce global memory traffic is to apply a **block tiling** technique that leverages **shared memory**. Since shared memory resides on-chip, it provides a much higher bandwidth than global memory and can lead to a significant improvement in applications with data reuse. One caveat is that shared memory has a much lower capacity compared to global memory; utilizing too much of shared memory can limit the number of blocks that can be scheduled by each SM.

<!-- START OF DIV -->
<div class="div-author-note">
<h4>Author's Note</h4>
As mentioned in my <a href="http://127.0.0.1:4000/cuda/2025/10/14/reduction_sum_part1.html">Reduction post</a>, I highly recommend watching <a href="https://www.nvidia.com/en-us/on-demand/session/gtcfall22-a41101/"><i>‚ÄúHow CUDA Programming Works‚Äù presentation by Stephen Jones</i></a>. It includes a great explanation and several examples showing how the resources required by each thread block directly affect the number of blocks that can be scheduled on each SM. Shared memory is one such resource to monitor.
</div>
<!-- END OF DIV -->

Additionally, I'd like to introduce the **three stages** of the implementation, which I also note as comments in my code:

1. Shared-memory stores
2. Dot-product computation
3. epilogue + output stores

Stage 1 and 2 are carried out within the block tile sliding through $K$, but otherwise, they are independent. For example, I can reuse the shared-memory stores from one kernel and the dot-product computation from another. Similarly, the epilogue stage is independent of the first two. You may also notice that many optimizations often modify some of the stages, not all.

For each `BLOCK_DIM x BLOCK_DIM` submatrix (tile) of $C$, the computation in Kernel 02 proceeds as follows.

<!-- START OF DIV -->
<div class="div-implementation">
<i>// Each iteration slides the tile <b>to the right</b> in $A$ and <b>downward</b> in $B$ across the $K$ dimension.</i>
<br />
<i>for k_offset = 0; k_offset < K; k_offset += BLOCK_DIM {</i>
<br /><br />
<ol>
<b>Stage 1: shared-memory stores</b>
<br /><br />
<li>
Each block allocates two `BLOCK_DIM x BLOCK_DIM` shared memory buffers: one for elements of $A$ (`A_shared`) and another for elements of $B$ (`B_shared`).
</li>
<li>
All threads collaboratively store a <code>BLOCK_DIM x BLOCK_DIM</code> tile of $A$ into <code>A_shared</code> corresponding to the same rows as $C$ and columns with range <code>[k_offset, k_offset + BLOCK_DIM)</code>.
</li>
<li>All threads collaboratively store a <code>BLOCK_DIM x BLOCK_DIM</code> tile of $B$ into <code>B_shared</code> corresponding to the same columns as $C$ and rows with range <code>[k_offset, k_offset + BLOCK_DIM)</code>.
</li>

<br />
<b>Stage 2: dot-product computation</b>
<br /><br />

<li>
Each thread initializes a register <code>sum</code> to store the partial dot product of <b>one cell</b> $c$ in the <code>BLOCK_DIM x BLOCK_DIM</code> submatrix of $C$.
</li>
<li>
Each thread computes and accumulates the partial dot product into the <code>sum</code> register.
</li>
</ol>

<i>}</i>

<br /><br />
<b>Stage 3: epilogue + output stores</b>
<br /><br />
<ol start="6">
<li>
Each thread computes $\alpha \times sum + \beta \times c_{value}$ and stores the result back to $C$.
</li>
</ol>
</div>
<!-- END OF DIV -->

The overall implementation can be illustrated below.

![image Kernel 02: Block tiling](/assets/images/2025-11-10-gemm/kernel2.png)
<p style="text-align: center;"><i>Kernel 02.</i></p>

And the kernel code is as follows.


```c++
template <uint const BLOCK_DIM>
__global__ void __launch_bounds__(BLOCK_DIM * BLOCK_DIM) block_tiling_gemm(
    int M, int N, int K,
    float alpha,
    float const *__restrict__ A,
    float const *__restrict__ B,
    float beta,
    float *__restrict__ C
) {
    __shared__ float As[BLOCK_DIM * BLOCK_DIM];
    __shared__ float Bs[BLOCK_DIM * BLOCK_DIM];

    {
        uint const block_row_offset{blockIdx.y * BLOCK_DIM};
        uint const block_col_offset{blockIdx.x * BLOCK_DIM};

        // Shift A, B, and C with the block offsets.
        A += block_row_offset * K;
        B += block_col_offset;
        C += block_row_offset * N + block_col_offset;
    }

    float sum{0.0f};

    for (uint k_offset{0}; k_offset < K; k_offset += BLOCK_DIM) {
        // Stage 1: shared-memory stores.
        As[threadIdx.y * BLOCK_DIM + threadIdx.x] = A[threadIdx.y * K + threadIdx.x];
        Bs[threadIdx.y * BLOCK_DIM + threadIdx.x] = B[threadIdx.y * N + threadIdx.x];
        __syncthreads();

        A += BLOCK_DIM;
        B += BLOCK_DIM * N;

        // Stage 2: dot-product computation.
        for (uint dot_idx{0}; dot_idx < BLOCK_DIM; ++dot_idx) {
            sum += As[threadIdx.y * BLOCK_DIM + dot_idx] * Bs[dot_idx * BLOCK_DIM + threadIdx.x];
        }
        __syncthreads();
    }

    // Stage 3: epilogue + output stores.
    C[threadIdx.y * N + threadIdx.x] = alpha * sum + beta * C[threadIdx.y * N + threadIdx.x];
}
```

<!-- START OF DIV -->
<div class="div-author-note">
<h4>Author's Note</h4>
I found the notion of shifting the pointers difficult to understand at first, so I would initially add the offsets within iterations, e.g.,
<br /><br />
<pre><code>uint const global_idx{(block_row_offset + threadIdx.y) * N + (block_col_offset + threadIdx.x)};
C[global_idx] = alpha * sum + beta * C[global_idx];
</code></pre>
Once I grasped it, while shifting the pointers does simplify the code, I found that adding offsets within iterations can sometimes yield higher throughput (particularly in the half-precision GEMM Kernel 12).
</div>
<!-- END OF DIV -->

With the tiling method described above, each element in matrix $A$ in global memory is now accessed only `N / BLOCK_DIM` times, and each element of matrix $B$ is accessed only `M / BLOCK_DIM` times. This reduction happens because each `BLOCK_DIM x BLOCK_DIM` tile is loaded into shared memory once and then reused by all threads in the block, significantly cutting down redundant global memory accesses. As a result, it leads to the following performance increase:

| Kernel # | Performance | % cuBLAS performance |
|:---|:---:|:---:|:---:|
| cuBLAS | 31.66 TFLOPs/s | 100% |
| kernel 01: naive | 2.05 TFLOPs/s | 6.47% |
| kernel 02: block tiling üÜï  | 3.49 TFLOPs/s | 11.02% |

<!-- START OF DIV -->
<div class="div-example">
<h4>Kernel 02 in the Roofline Model</h4>

Recall that Kernel 02 achieves a throughput of 3.49 TFLOPs/s. The <b>total floating-point operations</b> is the same as Kernel 01, which is

$$
2MN (K + 1) FLOPs
$$

The <b>total data transferred</b>, on the other hand, has <b>significantly decreased</b>. Each element in $A$ is now loaded from global memory only <code>N / BLOCK_DIM</code> times, and each element of $B$ is loaded only <code>M / BLOCK_DIM</code> times. Consequently, the total global memory accesses are:

$$
A: M \times K \times \frac{N}{BLOCK\_DIM}, \, B: K \times N \times \frac{M}{BLOCK\_DIM}
$$

Combined with the global memory access of $C$, the overall global memory traffic becomes: 

$$
(\frac{2MNK}{BLOCK\_DIM} + 2MN) \times 4 \, Bytes
$$

Plugging in $M = N = K = 4096$ and a <code>BLOCK_DIM</code> of $32$, the <b><i>theoretical</i> operational intensity</b> rises to approximately 7.94 FLOPs/B. 
</div>
<!-- END OF DIV -->

While Kernel 02 remains memory-bound, we can already see a shift from Kernel 01 toward the compute-bound region. To push it further, we need to increase the operational intensity even more. One effective approach is **thread coarsening**, i.e., allowing each thread to perform more work, which in this case, can further reduce global memory traffic and improve overall performance.

<hr />

#### Kernel 03: 2D thread coarsening

This kernel corresponds to Kernel 5 in [Simon's blog post](https://siboehm.com/articles/22/CUDA-MMM). We now define several new terms:

1. Block-level
    - `BM` corresponds to the number of rows processed by each block
    - `BN` corresponds to the number of columns processed by each block
    - `BK` contributes to the shared memory size: `As` uses $BM \times BK$ slots and `Bs` uses $BK \times BN$ slots.

2. Thread-level
    - `TM` corresponds to the number of rows processed by each thread
    - `TN` corresponds to the number of columns processed by each thread

The kernel implementation is illustrated below.

![image Kernel 01: Naive](/assets/images/2025-11-10-gemm/kernel3.png)
<p style="text-align: center;"><i>Kernel 03.</i></p>

This is where I had to remind myself many times that **stage 1 (shared-memory stores) and stage 2 (dot-product computation) are independent of each other** (it's probably because each shared-memory buffer was the same size as the block in the previous kernel). **This kernel primarily modifies stage 2 and 3 given the `TM x TN` tile to process**. The only change to stage 1 results from transforming the thread block into one dimension; since the coordinates used in stage 1 and stage 2 (even the thread coordinates for $A$ vs $B$ in stage 2) are now different, it's easier to work with one-dimensional thread block and generate the coordinates accordingly.

In other words, instead of defining a 2D block shape like this:

```c++
dim3 block_dim(BN / TN, BM / TM);
```

we now switch to a 1D layout:

```c++
dim3 block_dim((BM * BN) / (TM * TN));
```

This simplifies the indexing logic while still covering the same number of threads.

My kernel implementation is as follows.

```c++
template <uint const NUM_THREADS, uint const BM, uint const BN, uint const BK, uint const TM, uint const TN>
__global__ void __launch_bounds__(NUM_THREADS) thread_coarsening_2d_gemm(
    int M, int N, int K, float alpha,
    float const *__restrict__ A,
    float const *__restrict__ B,
    float       beta,
    float       *__restrict__ C
) {
    __shared__ float As[BM * BK];
    __shared__ float Bs[BK * BN];

    // CHANGE 1: each thread now computes TM x TN tile.
    float out_values[TM * TN] = {0.0f};

    // CHANGE 2: since the block now has 1D layout, obtain the x and y coordinates
    // within BM x BN (for dot product computation & output stages).
    uint const threadIdx_x{threadIdx.x % (BN / TN)};
    uint const threadIdx_y{threadIdx.x / (BN / TN)};

    ...

    // CHANGE 3.1: compute the number of iterations each thread stores shared memory elements;
    // used only in the shared-memory stores.
    constexpr uint stride_A{NUM_THREADS / BK};
    constexpr uint stride_B{NUM_THREADS / BN};

    // CHANGE 3.2: obtain the row and column indices of A and B during shared-memory stores.
    uint const A_block_row_idx{threadIdx.x / BK};
    uint const A_block_col_idx{threadIdx.x % BK};
    uint const B_block_row_idx{threadIdx.x / BN};
    uint const B_block_col_idx{threadIdx.x % BN};

    for (int k_offset{0}; k_offset < K; k_offset += BK) {
        // Stage 1: shared-memory stores.
        for (int A_load_offset{0}; A_load_offset < BM; A_load_offset += stride_A) {
            As[(A_block_row_idx + A_load_offset) * BK + A_block_col_idx] =
                A[(A_block_row_idx + A_load_offset) * K + A_block_col_idx];
        }
        for (int B_load_offset{0}; B_load_offset < BK; B_load_offset += stride_B) {
            Bs[(B_block_row_idx + B_load_offset) * BN + B_block_col_idx] =
                B[(B_block_row_idx + B_load_offset) * N + B_block_col_idx];
        }
        __syncthreads();

        A += BK;
        B += BK * N;

        // Stage 2: dot-product computation.
        for (int k{0}; k < BK; ++k) {
            for (int tile_y_idx{0}; tile_y_idx < TM; ++tile_y_idx) {
                for (int tile_x_idx{0}; tile_x_idx < TN; ++tile_x_idx) {
                    out_values[tile_y_idx * TN + tile_x_idx] +=
                        As[(threadIdx_y * TM + tile_y_idx) * BK + k] *
                        Bs[k * BN + (threadIdx_x * TN + tile_x_idx)];
                }
            }
        }
        __syncthreads();
    }

    // Stage 3: output stores.
    for (int tile_y_idx{0}; tile_y_idx < TM; ++tile_y_idx) {
        for (int tile_x_idx{0}; tile_x_idx < TN; ++tile_x_idx) {
            uint C_idx{};
            {
                uint const cell_row_idx{threadIdx_y * TM + tile_y_idx};
                uint const cell_col_idx{threadIdx_x * TN + tile_x_idx};
                C_idx = cell_row_idx * N + cell_col_idx;
            }
            C[C_idx] =
                alpha * out_values[tile_y_idx * TN + tile_x_idx] +
                beta * C[C_idx];
        }
    }
}
```

**Iterations during shared-memory stores**

In many cases, the shared-memory tiles are larger than the number of threads in a block. In other words, `BM x BK` and `BK x BN` may exceed the block size. When that happens, each thread must perform multiple iterations to fully populate the shared memory. The strides added in each iteration are defined by `stride_A` and `stride_B`, which allow the kernel to loop across `BM` to fill `As` and `BK` to fill `Bs`. For simplicity, we assume that

- `(BM x BK) % NUM_THREADS == 0`
- `(BK x BN) % NUM_THREADS == 0`
- `NUM_THREADS % BK == 0`
- `NUM_THREADS % BN == 0`

In practice, however, we'll need proper boundary checks to handle cases where these conditions don't hold.

![image Shared-memory stores in Kernel 03](/assets/images/2025-11-10-gemm/kernel3_sm_store.png)
<p style="text-align: center;"><i>An example of shared-memory stores in Kernel 03.</i></p>

This kernel achieves the following performance, marking a significant jump compared to the previous implementation.

| Kernel # | Performance | % cuBLAS performance |
|:---|:---:|:---:|:---:|
| cuBLAS | 31.66 TFLOPs/s | 100% |
| kernel 01: naive | 2.05 TFLOPs/s | 6.47% |
| kernel 02: block tiling | 3.49 TFLOPs/s | 11.02% |
| kernel 03: 2D thread coarsening üÜï | 20.05 TFLOPs/s | 63.33% |

<hr />

#### Kernel 04: Vectorized memory access

Global memory access is relatively slow, so it's generally recommended to **combine multiple small transfers into a single larger transfer**. This approach reduces the overhead associated with each memory access. According to the [CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#maximize-memory-throughput), global memory can be accessed using **32-, 64-, or 128-bit memory transactions**.

Up to now, all global memory loads have been performed using 32-bit transactions. In Kernel 3, for example, if we inspect the line where elements from `A` are loaded in [Godbolt](https://godbolt.org/z/a4sc1Wfb1), we see the PTX instruction: `ld.global.f32`. The goal of the optimization in this kernel is to **reduce the number of global memory transfers** by merging them into **128-bit transactions**, which improves memory throughput. To do that, we use the built-in vector type `float4`, which packs four consecutive `float` elements into a single 16-byte vector.

Additionally, we introduce another level of caching using registers in Stage 2 (dot-product computation). We discussed earlier that shared memory has higher bandwidth compared to global memory. Register access is even faster than shared memory, so since we are reusing elements in shared memory during the `TM x TN` tile computation, we can store the elements of each column from As and each row from Bs in registers. This technique also helps understanding how matrices are loaded into fragments when using tensor cores later.

The code update is as follows.

**Stage 1: shared-memory stores**

```c++
// FACTOR = log2(the number of elements packed together).
// In this case, FACTOR = 2 (4 float elements in float4).
constexpr uint stride_A{(NUM_THREADS << FACTOR) / BK};
constexpr uint stride_B{(NUM_THREADS << FACTOR) / BN};

uint const A_block_row_idx{thread_idx / (BK >> FACTOR)};
uint const A_block_col_idx{(thread_idx % (BK >> FACTOR)) << FACTOR};
uint const B_block_row_idx{thread_idx / (BN >> FACTOR)};
uint const B_block_col_idx{(thread_idx % (BN >> FACTOR)) << FACTOR};

for (int A_load_offset{0}; A_load_offset < BM; A_load_offset += stride_A) {
    // Reinterpret the bit pattern of 4 elements of floats as a float4 element.
    reinterpret_cast<float4 *>(&As[(A_block_row_idx + A_load_offset) * BK + A_block_col_idx])[0] =
        reinterpret_cast<float4 *>(&A[(A_block_row_idx + A_load_offset) * K + A_block_col_idx])[0];
}
    
for (int B_load_offset{0}; B_load_offset < BK; B_load_offset += stride_B) {
    reinterpret_cast<float4 *>(&Bs[(B_block_row_idx + B_load_offset) * BN + B_block_col_idx])[0] =
        reinterpret_cast<float4 *>(&B[(B_block_row_idx + B_load_offset) * N + B_block_col_idx])[0];
}
```

**Stage 2: dot-product computation**

This is where the register-level caching is performed.

```c++
// Note that the thread offsets have been added to As and Bs.
for (int k{0}; k < BK; ++k) {
    for (int tile_y_idx{0}; tile_y_idx < TM; ++tile_y_idx)
        reg_M[tile_y_idx] = As[tile_y_idx * BK + k];
        
    for (int tile_x_idx{0}; tile_x_idx < TN; ++tile_x_idx)
        reg_N[tile_x_idx] = Bs[k * BN + tile_x_idx];
    
    for (int tile_y_idx{0}; tile_y_idx < TM; ++tile_y_idx) {
        for (int tile_x_idx{0}; tile_x_idx < TN; ++tile_x_idx) {
            out_values[tile_y_idx * TN + tile_x_idx] +=
                reg_M[tile_y_idx] * reg_N[tile_x_idx];
        }
    }
}
```

**Stage 3: epilogue + output stores**

```c++
for (int tile_y_idx{0}; tile_y_idx < TM; ++tile_y_idx) {
    // Notice that we accumulate tile_x_idx by 4 here.
    for (int tile_x_idx{0}; tile_x_idx < TN; tile_x_idx += 4) {
        uint const cell_row_idx{threadIdx_y * TM + tile_y_idx};
        uint const cell_col_idx{threadIdx_x * TN + tile_x_idx};

        float4 tmp = reinterpret_cast<float4 *>(
            &C[cell_row_idx * N + cell_col_idx]
        )[0];

        tmp.x = alpha * out_values[tile_y_idx * TN + tile_x_idx + 0] + beta * tmp.x;
        tmp.y = alpha * out_values[tile_y_idx * TN + tile_x_idx + 1] + beta * tmp.y;
        tmp.z = alpha * out_values[tile_y_idx * TN + tile_x_idx + 2] + beta * tmp.z;
        tmp.w = alpha * out_values[tile_y_idx * TN + tile_x_idx + 3] + beta * tmp.w;
        reinterpret_cast<float4 *>(
            &C[cell_row_idx * N + cell_col_idx]
        )[0] = tmp;
    }
}
```

The updated PTX instructions are available in [this Godbolt link](https://godbolt.org/z/a5ETsWxha). Notably, the global memory load/store operations now use `ld.global.nc.v4.u32` for $A$ and $B$ memory loads, `ld.global.v4.u32` for $C$ memory loads, and`st.global.v4.f32` for $C$ memory stores. The shared memory stores, on the other hand, now use `st.shared.v4.u32`. These changes confirm that the kernel now performs 16-byte (or 128-bit) vectorized memory transfers. 

Note that `.nc` in `ld.global.nc.v4.u32` stands for *non-coherent cache*. It indicates that the global memory being loaded is treated as read-only, i.e., the hardware does not need to maintain immediate coherence with global memory stores across SMs. This allows the GPU to optimize the data-fetch operations. The compiler likely infers that $A$ and $B$ are read-only because there are no global stores to these arrays. We can also increase the likelihood of the compiler detecting read-only data if we use both `const` and `__restrict__`; unfortunately, we can't use `const` on $A$ and $B$ here since the `reinterpret_cast` operator cannot cast away const. Read more on how the compiler handles potentially read-only global memory data [here](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#global-memory-5-x).

We now achieve 26.01 TFLOP/s, which is about 80% of the cuBLAS performance!

| Kernel # | Performance | % cuBLAS performance |
|:---|:---:|:---:|:---:|
| cuBLAS | 31.66 TFLOPs/s | 100% |
| kernel 01: naive | 2.05 TFLOPs/s | 6.47% |
| kernel 02: block tiling | 3.49 TFLOPs/s | 11.02% |
| kernel 03: 2D thread coarsening | 20.05 TFLOPs/s | 63.33% |
| kernel 04: vectorized memory access üÜï | 26.16 TFLOPs/s | 82.63% |

<hr />

#### Kernel 05: Warp tiling

I initially struggled to understand Simon's Kernel 10: Warp Tiling, so I started by implementing my own version based on how I interpreted the notion of warp tiling. This process helped me understand how Simon introduces an additional split within the warp to **maximize instruction-level parallelism**--this is where the thread tiling comes into play as mentioned in the post.

I refer to the non-subdivided warp tiling (i.e., my initial interpretation of warp tiling) as Kernel 5, and the subdivided version (Simon's warptiling kernel) as Kernel 6.

Threads are always created, scheduled, and executed in groups of 32 called **warps**. When a multiprocessor receives one or more thread blocks to execute, it splits them into warps, and each warp is managed by the **warp scheduler**. In a warp tiling kernel, we restructure the work *within a warp* so that all threads cooperate to compute a tile of $C$, which can further boost performance.

For example, when threads in a warp access contiguous memory addresses, global memory loads can be coalesced, and potential shared-memory bank conflicts become easier to detect and avoid (these conflicts occur when threads in a warp access the same shared memory bank but different addresses; see my [Sum Reduction post Kernel 1]({% link _posts/2025-10-27-reduction_sum_part2.markdown %}#kernel-1-interleaved-addressing-with-thread-divergence-resolved) for examples). Most importantly, the warp-level tile structure and load/compute pattern lays the foundation for using **tensor cores** in subsequent optimization steps.

In Kernel 5, we introduce the following terms:

- `WM` corresponds to the number of rows processed by each warp
- `WN` corresponds to the number of columns processed by each warp

One limitation is that each warp always consists of **32** threads (older GPUs may only have 16 threads per warp). This leads to the following constraints:

- The total number of warps per block: `NUM_THREADS / 32 = (BM x BN) / (WM x WN)`
- The total number of threads per warp: `32 = (WM x WN) / (TM x TN)`

The following figure illustrates Kernel 05.

![Kernel 05](/assets/images/2025-11-10-gemm/kernel5.png)
<p style="text-align: center;"><i>Kernel 05.</i></p>

<!-- START OF DIV -->
<div class="div-author-note">
<h4>Author's Note</h4>
I agree with Simon that drawing the illustration helps with understanding the kernel implementation, especially when dealing with more complex methods. While the illustration above is more simplified, I would recommend incorporating $A$ and $B$ (or <code>As</code> and <code>Bs</code> in this case) at each level if things are still unclear, similar to how I illustrate Kernel 04 or how Simon illustrates each kernel.
<br /><br />
I ended up drawing the illustrations for Kernel 05 and Kernel 06 so many times on papers while trying to explain the implementation to myself out loud (though I also personally love to draw them; pretty diagrams make me happy ‚ù§Ô∏è).
</div>
<!-- END OF DIV -->

There is no change to stage 1 (shared-memory stores). You'd notice that [I reuse the shared-memory stores function from Kernel 04 in my code](https://github.com/kathsucurry/cuda_matrix_multiplication/blob/main/src/kernels_fp32/05_warptiling.cuh#L105).

When it comes to stage 2 (dot-product computation) and stage 3 (epilogue + output stores), aside from the thread-level offsets (e.g., `thread row index * TM` to get the thread row offset), we now have to consider the warp-level offsets as well. This is done by introducing the following variables:

1. `lane_idx`: the index of a thread *within its warp*, computed as `threadIdx.x % 32`.
2. `warp_idx`: the index of the warp within its block, computed as `threadIdx.x / 32`.

The warp-level and thread-level offsets can now be computed as follows.

```c++
uint const lane_idx{threadIdx.x % 32};
uint const warp_idx{threadIdx.x / 32};

// BN / WN computes the number of warps horizontally.
uint const warp_row_offset{(warp_idx / (BN / WN)) * WM};
uint const warp_col_offset{(warp_idx % (BN / WN)) * WN};

// WN / TN computes the number of thread tiles within a warp horizontally.
uint const thread_row_offset{(lane_idx / (WN / TN)) * TM};
uint const thread_col_offset{(lane_idx % (WN / TN)) * TN};
```

Once we update the offsets when shifting $A$, $B$, and $C$'s pointers, the rest of the implementation is exactly the same as the previous kernel! *That is the beauty of shifting the pointers approach~*

Performance-wise, however, we don't really see much differences  between Kernel 04 and Kernel 05:

| Kernel # | Performance | % cuBLAS performance |
|:---|:---:|:---:|:---:|
| cuBLAS | 31.66 TFLOPs/s | 100% |
| kernel 01: naive | 2.05 TFLOPs/s | 6.47% |
| kernel 02: block tiling | 3.49 TFLOPs/s | 11.02% |
| kernel 03: 2D thread coarsening | 20.05 TFLOPs/s | 63.33% |
| kernel 04: vectorized memory access | 26.16 TFLOPs/s | 82.63% |
| kernel 05: warp tiling üÜï | 26.19 TFLOPs/s | 82.72% |

<hr />

#### Kernel 06: Warp tiling, subdivided

Kernel 05 has the following tiling hierarchy: `block tile (BM x BN) --> warp tile (WM x WN) --> thread tile (TM x TN)`. In Kernel 06, we add another level between `warp tile` and `thread tile`. Let's call the new hierarchy level `subwarp tile`.

We also introduce some new terms (the same terms used in Simon's):

1. `WMITER`: The number of subwarps within a warp vertically.
2. `WNITER`: The number of subwarps within a warp horizontally.
3. `WSUBM`: The number of rows within a subwarp.
4. `WSUBN`: The number of columns within a subwarp.

The following figure illustrates the difference between Kernel 05 and Kernel 06.

![Kernel 05 vs Kernel 06](/assets/images/2025-11-10-gemm/kernel6.png)
<p style="text-align: center;"><i>Kernel 05 vs Kernel 06.</i></p>

Since each warp consists of 32 threads, the following requirement must apply.

$$
\frac{WM \times WN}{WMITER \times TM \times WNITER \times TN} = 32
$$

Additionally, each thread now computes `WMITER * TM * WNITER * TN` output elements, compared to only `TM * TN` output elements in the previous kernel. Some tunings, especially on `TM` and `TN` might be necessary.

Stage 1 (shared-memory stores) remains the same as in the previous kernel. In stage 2 (dot-product computation) and stage 3 (epilogue + output stores), we simply place the subwarp loop on top of the existing thread-tiling loop:

**Stage 2 (dot-product computation)**

```c++
// reg_M is now [WMITER * TM] and reg_N is [WNITER * TN].
for (int k{0}; k < BK; ++k) {
    for (int wmiter_idx{0}; wmiter_idx < WMITER; ++wmiter_idx) {
        for (int tm_idx{0}; tm_idx < TM; ++tm_idx) {
            reg_M[wmiter_idx * TM + tm_idx] = As[
                (wmiter_idx * WSUBM + tm_idx) * BK + k
            ];
        }
    }

    for (int wniter_idx{0}; wniter_idx < WNITER; ++wniter_idx) {
        for (int tn_idx{0}; tn_idx < TN; ++tn_idx) {
            reg_N[wniter_idx * TN + tn_idx] = Bs[
                k * BN + (wniter_idx * WSUBN + tn_idx)
            ];
        }
    }

    for (int wmiter_idx{0}; wmiter_idx < WMITER; ++wmiter_idx)
        for (int wniter_idx{0}; wniter_idx < WNITER; ++wniter_idx)
            for (int tm_idx{0}; tm_idx < TM; ++tm_idx)
                for (int tn_idx{0}; tn_idx < TN; ++tn_idx) {
                    out_values[(wmiter_idx * TM + tm_idx) * (WNITER * TN) + wniter_idx * TN + tn_idx] +=
                        reg_M[wmiter_idx * TM + tm_idx] * reg_N[wniter_idx * TN + tn_idx];
                }
}
```

**Stage 3 (epilogue + output stores)**

```c++
for (int wmiter_idx{0}; wmiter_idx < WMITER; ++wmiter_idx)
    for (int wniter_idx{0}; wniter_idx < WNITER; ++wniter_idx) {
        uint const tile_row_idx{wmiter_idx * WSUBM};
        uint const tile_col_idx{wniter_idx * WSUBN};
            
        for (int tm_idx{0}; tm_idx < TM; ++tm_idx)
            for (int tn_idx{0}; tn_idx < TN; tn_idx += 4) {
                uint const cell_row_idx{tile_row_idx + tm_idx};
                uint const cell_col_idx{tile_col_idx + tn_idx};
                    
                float4 tmp = reinterpret_cast<float4 *>(
                    &C[cell_row_idx * N + cell_col_idx]
                    )[0];
                uint const first_out_idx = (wmiter_idx * TM + tm_idx) * (WNITER * TN) + wniter_idx * TN + tn_idx;
                tmp.x = alpha * out_values[first_out_idx + 0] + beta * tmp.x;
                tmp.y = alpha * out_values[first_out_idx + 1] + beta * tmp.y;
                tmp.z = alpha * out_values[first_out_idx + 2] + beta * tmp.z;
                tmp.w = alpha * out_values[first_out_idx + 3] + beta * tmp.w;
                reinterpret_cast<float4 *>(
                    &C[cell_row_idx * N + cell_col_idx]
                )[0] = tmp;
            }
    }     
```

These modifications lead to a performance of 28.70 TFLOPs/s, which is more than 90% of cuBLAS!

| Kernel # | Performance | % cuBLAS performance |
|:---|:---:|:---:|:---:|
| cuBLAS | 31.66 TFLOPs/s | 100% |
| kernel 01: naive | 2.05 TFLOPs/s | 6.47% |
| kernel 02: block tiling | 3.49 TFLOPs/s | 11.02% |
| kernel 03: 2D thread coarsening | 20.05 TFLOPs/s | 63.33% |
| kernel 04: vectorized memory access | 26.16 TFLOPs/s | 82.63% |
| kernel 05: warp tiling | 26.19 TFLOPs/s | 82.72% |
| kernel 06: warp tiling, subdivided üÜï | 28.70 TFLOPs/s | 90.65% |

*How does Kernel 06 lead to such performance improvements?*

If we look at my parameters setup, we'll see that each thread in both Kernel 05 and Kernel 06 compute the same amount of output elements . In particular, each thread computes 128 output elements in both configurations:

| Kernel # | NUM_THREADS | WM | WN | TM | TN | WMITER | WNITER | 
|:---|:---:|:---:|:---:|:---|:---:|:---:|:---:|
| Kernel 05 | 128 | 32 | 128 | 16 | 8 | 1 | 1 |
| Kernel 06 | 128 | 32 | 128 | 8 | 4 | 2 | 2 |

<!-- START OF DIV -->
<div class="div-author-note">
<h4>Author's Note</h4>
As a precaution, I also ran Kernel 06 with the Kernel 05 configuration above, and it produced performance very similar to what's shown in the performance table.
</div>
<!-- END OF DIV -->

It's fascinating to observe that the throughput of Kernel 06 is approximately 10% higher than Kernel 05. One noticeable difference identified when profiling Kernel 05 vs Kernel 06 using Nsight is that Kernel 06 has 12.84% `instruction per cycles (IPC) elapsed` higher than Kernel 05.

If we examine [the PTX instructions for each `K` iteration](https://godbolt.org/z/49PPss8hd) in the `compute_dot_products()` function (where I use Kernel 06 implementation code for both kernels but with different configurations), we can see that increasing both `WMITER` and `WNITER` from 1 to 2 changes the ordering of the PTX instructions:

![PTX instructions: Kernel 05 vs Kernel 06](/assets/images/2025-11-10-gemm/kernel6_ptx.png)
<p style="text-align: center;"><i>PTX instructions: Kernel 05 vs Kernel 06.</i></p>

The `fma` (fused multiply-add) instruction can only execute once *the data it depends on is already available* in registers. In Kernel 05, all eight column values of `Bs` must be loaded into `reg_N` (since `TN = 8` in the Kernel 05 configuration) before the `fma` instructions for subsequent rows can be issued. This creates large `fma` blocks, each containing eight `fma` instructions. These long dependency chains introduce longer stalls, which reduce instruction-level parallelism (ILP) and ultimately lower IPC.

On the other hand, in Kernel 06, the `fma` instructions for subsequent rows only need to wait for the first four column elements of `Bs` to be loaded into `reg_N` (since `TN = 4` in the Kernel 06 configuration). As a result, each `fma` block is smaller, containing only four `fma` instructions. After the first `TM * TN` FMAs complete, we then see the next four `reg_N` stores begin. Additionally, the PTX instructions in Kernel 06 also include **blocks of fully independent FMA instructions** with no preceding register loads, which can execute immediately and provide pure ILP, further boosting IPC.

Another way to illustrate the PTX instruction ordering is shown in the diagram below.

![PTX instructions diagram: Kernel 05 vs Kernel 06](/assets/images/2025-11-10-gemm/kernel6_ptx_diagram.png)
<p style="text-align: center;"><i>PTX instruction diagram: Kernel 05 vs Kernel 06.</i></p>

Out of curiosity, I experimented with different `TM`, `TN`, `WMITER`, and `WNITER` configurations while keeping the total number of elements computed per thread the same (i.e., 128 elements). The stage 3 (epilogue + output stores) configuration was kept the same  throughout these experiments. I did, however, have to disable the matrix verification step temporarily since the differing configurations would otherwise produce incorrect outputs.

| TM | TN | WMITER | WNITER | TFLOPs/s | Remark |
|:---|:---:|:---:|:---:|:---|:---:|
| 8 | 4 | 1 | 4 | 25.39 | |
| 8 | 4 | 2 | 2 | 28.70 | Original configuration |
| 8 | 4 | 4 | 1 | 21.53 | |
| 4 | 4 | 1 | 8 | 21.53 | |
| 4 | 4 | 2 | 4 | 25.41 | |
| 4 | 4 | 4 | 2 | 28.73 | Very similar to our original's performance! |
| 4 | 4 | 8 | 1 | 21.52 | |
| 4 | 8 | 1 | 4 | 21.51 | |
| 4 | 8 | 2 | 2 | 22.30 | |
| 4 | 8 | 4 | 1 | 26.23 | |
| 1 | 16 | 1 | 8 | 15.11 | |
| 1 | 16 | 2 | 4 | 19.99 | |
| 1 | 16 | 4 | 2 | 15.66 | |
| 1 | 16 | 8 | 1 | 15.59 | |

If we group the experiments by `TN` and look at the the best-performing configuration in each group, we see that larger `TN` values lead to worse performance. This is expected because a larger `TN` creates a longer dependency chain: more column elements must be loaded into `reg_N` before FMAs for subsequent rows can begin.

Focusing on the experiments where `TN = 4`, we observe that performance increases as `WNITER` decreases, reaches a peak, and then eventually drops again. When `WNITER = 1`, there are no blocks of fully independent FMA instructions, which likely explains the performance drop.

However, a larger `WNITER` does not always guarantee better performance. One possible explanation is that increasing `WNITER` enlarges the overall block of alternating register-store and FMA instructions. As these blocks grow, they may introduce new dependency chains and ultimately limiting ILP.

<hr />

#### Kernel 07: Transpose `As`

One optimization recommended in Simon's post is to vectorize the shared-memory loads as well. Vectorizing the loads from `Bs` is straightforward because `Bs` is always accessed horizontally. In contrast, vectorizing the loads from `As` requires transposing `As` since its elements are currenty loaded vertically.

When storing into a transposed `As`, we can still vectorize the global-memory loads from $A$ using `float4`, and then manually distribute the four elements into the appropriate rows of the transposed shared-memory buffer:

```c++
// Part of load_gmem_to_smem() function.
for (int A_load_offset{0}; A_load_offset < BM; A_load_offset += stride_A) {
    // Keep the vectorized loads.
    float4 tmp = reinterpret_cast<float4 *>(&A[(A_block_row_idx + A_load_offset) * K + A_block_col_idx])[0];

    As[(A_block_col_idx + 0) * BM + A_block_row_idx + A_load_offset] = tmp.x;
    As[(A_block_col_idx + 1) * BM + A_block_row_idx + A_load_offset] = tmp.y;
    As[(A_block_col_idx + 2) * BM + A_block_row_idx + A_load_offset] = tmp.z;
    As[(A_block_col_idx + 3) * BM + A_block_row_idx + A_load_offset] = tmp.w;
}
```

We then vectorize all shared memory loads as follows.

```c++
for (int k{0}; k < BK; ++k) {
    for (int wmiter_idx{0}; wmiter_idx < WMITER; ++wmiter_idx) {
        // Notice that we now add 4 to tm_idx each time.
        for (int tm_idx{0}; tm_idx < TM; tm_idx += 4) {
            reinterpret_cast<float4 *>(&reg_M[wmiter_idx * TM + tm_idx])[0] =
                reinterpret_cast<float4 *>(&As[k * BM + (wmiter_idx * WSUBM + tm_idx)])[0];
        }
    }
    for (int wniter_idx{0}; wniter_idx < WNITER; ++wniter_idx) {
        // Notice that we now add 4 to tn_idx each time.
        for (int tn_idx{0}; tn_idx < TN; tn_idx += 4) {
            reinterpret_cast<float4 *>(&reg_N[wniter_idx * TN + tn_idx])[0] =
                reinterpret_cast<float4 *>(&Bs[k * BN + (wniter_idx * WSUBN + tn_idx)])[0];
        }
    }

    ...

}
```

After applying these modifications, generating the PTX instructions should now show `ld.shared.v4.f32` when loading from shared memory instead of `ld.shared.f32`. 

Overall, transposing `As` leads to a noticeable improvement compared to Kernel 06!

| Kernel # | Performance | % cuBLAS performance |
|:---|:---:|:---:|:---:|
| cuBLAS | 31.66 TFLOPs/s | 100% |
| kernel 01: naive | 2.05 TFLOPs/s | 6.47% |
| kernel 02: block tiling | 3.49 TFLOPs/s | 11.02% |
| kernel 03: 2D thread coarsening | 20.05 TFLOPs/s | 63.33% |
| kernel 04: vectorized memory access | 26.16 TFLOPs/s | 82.63% |
| kernel 05: warp tiling | 26.19 TFLOPs/s | 82.72% |
| kernel 06: warp tiling, subdivided | 28.70 TFLOPs/s | 90.65% |
| kernel 07: transpose `As` üÜï | 29.71 TFLOPs/s | 93.84% |

<!-- START OF DIV -->
<div class="div-author-note">
<h4>Author's Note</h4>
I did experiment with only vectorizing <code>Bs</code> loads in Kernel 06; it did not lead to any significant performance improvement.
</div>
<!-- END OF DIV -->

*How does transposing `As` affect shared memory bank conflicts?*

Recall that shared memory is divided into 32 equally-sized banks where each bank can handle a 4-byte load or store operation per clock cycle. Consecutive 4-byte words are assigned to consecutive banks, so in our case, each `float` element occupies one bank.

**`As` stores**

In the previous kernels, each thread-block stores multiple `float4` elements loaded from $A$ into a shared-memory buffer `As` of size $[BM \times BK]$. With my current $BK$ set to 32, elements from the same column but different rows in `As` are placed in the same bank. Since each thread stores a `float4` element, its four `float` components now occupy four successive banks. In other words, every eight threads together cover all 32 banks. Because a warp contains 32 threads, a warp would cover four consecutive rows, which results in **4-way bank conflicts**.

Now that `As` is transposed, instead of storing into consecutive indices horizontally, we now store the elements into `As` vertically. The four rows that were initially covered by a warp now become four columns where each column is of size $BK$. Since my $BM$ value is also divisible by 32, i.e., the elements in each column of the transposed buffer are assigned to the same bank, we now have **`BK`-way bank conflicts**, which is 32-way bank conflicts in my case...

**`As` loads**

![How elements in As are initially loaded](/assets/images/2025-11-10-gemm/kernel7.png)
<p style="text-align: center;"><i>The <b>first</b> elements loaded from non-transposed <code>As</code> by all threads in a warp during the <code>k</code>th, <code>WMITER = 0</code>, <code>WNITER = 0</code> iteration.</i></p>

In each iteration over `K`, `WMITER`, and `WNITER`, all threads in a warp compute one element from each `TM * TN` block. As a result, the number of elements a warp needs to load from `As` equals the number of `TM * TN` blocks stacked vertically. In my configuration where `WM = 32`, `WMITER = 2`, `TM = 8`, this results in 2-way bank conflicts.

Once `As` is transposed, the number of elements loaded from `As` remains the same, but they are now distributed across different columns. Because of this rearrangement, **bank conflicts are eliminated**.

**Summary**

With no change to `Bs` stores and loads, it is interesting to see that although bank conflicts on `As` appear to be more severe overall in this kernel, vectorizing all shared memory loads and register stores still leads to performance improvements.

<hr />

#### Kernel 08: Asynchronous global-memory loads + double buffering

As the name suggests, this kernel applies 2 optimizations:
1. asynchronous global-memory loads, and
2. double buffering.

**Asynchronous Data Copies (details taken from [this presentation](https://www.nvidia.com/en-us/on-demand/session/gtc24-s62192/))**

So far, we have been using **synchronous** data copies when loading elements from global memory and storing them "directly" into shared memory, e.g., when we run `smem[index] = gmem[index]`. At the hardware level, however, the data is actually first copied from global memory into registers through the L1 cache, and only then written from registers into shared memory. Consequently, we end up wasting both registers and L1 bandwidth.

Introduced as part of Asynchronous Data Copies in CUDA 11.0, `__pipeline_memcpy_async` is a `memcpy_async` primitive that allows data to be copied from global memory to shared memory **without passing through registers**. Depending on the transfer size, it can even bypass the L1 cache. By avoiding the register path, fewers registers are required, which can in turn improve occupancy.

![Global to shared memory copies](/assets/images/2025-11-10-gemm/kernel8_intro.png)
<p style="text-align: center;"><i>Synchronous vs asynchronous data copies from global to shared memory.</i></p>

To use asynchronous data copies, we simply replace the stage-1 (shared-memory store) code from **Kernel 06** with the code below. We don't use transposed `As` here since the current function does not appear to support transposed layouts.

```c++
// Make sure to include cuda_pipeline.h.
for (int A_load_offset{0}; A_load_offset < BM; A_load_offset += stride_A) {
    __pipeline_memcpy_async(
        &As[(A_block_row_idx + A_load_offset) * BK + A_block_col_idx],
        &A[(A_block_row_idx + A_load_offset) * K + A_block_col_idx],
        4 * sizeof(float)
    );
}
        
for (int B_load_offset{0}; B_load_offset < BK; B_load_offset += stride_B) {
    __pipeline_memcpy_async(
        &Bs[(B_block_row_idx + B_load_offset) * BN + B_block_col_idx],
        &B[(B_block_row_idx + B_load_offset) * N + B_block_col_idx],
        4 * sizeof(float)
    );
}
__pipeline_commit();
```

Although not shown here, using the code above might already yield some improvements. We will isolate and examine this improvement in the half-precision GEMM implementations Kernel 10.

**Double buffering**

For each sliding tile with length $BK$ across $K$, we currently perform shared-memory stores (stage 1) that is immediately followed by dot-product computation (stage 2) of *the particular tile* (see the illustration below).

![The pipeline of the previous kernels](/assets/images/2025-11-10-gemm/kernel8_intro_prev.png)
<p style="text-align: center;"><i>The pipeline of the previous kernels.</i></p>

To further hide the data transfer latency, we can overlap the computation with data transfer illustrated as follows.

![The new pipeline in Kernel 09](/assets/images/2025-11-10-gemm/kernel8_intro_now.png)
<p style="text-align: center;"><i>The pipeline we aim to use in Kernel 09 to hide data transfer latency.</i></p>

To enable this, we introduce an additional shared-memory buffer: one buffer holds the data currently used for computation, while the other preloads the next data tile, hence, the name **double buffering**.

You can find the pseudocode below.

```c++
__shared__ float As[2][BM * BK];
__shared__ float Bs[2][BK * BN];

// Load tile 0 elements from A and B into the first buffer of As and Bs.
load_gmem_to_smem(tile[0], A, B, As[0], Bs[0]);

// Define which shared-memory buffer is to be used for computation.
int current{0};

for (tile_index = 1; tile_index < TILE_NUM; ++tile_index) {
    // Load the next tile elements from A and B into the other buffer of As and Bs.
    load_gmem_to_smem(tile[tile_index], A, B, As[current ^ 1], Bs[current ^ 1]);

    // Wait for the second last shared-memory stores to finish (i.e., the current tile).
    __pipeline_wait_prior(1);
    // Make sure all threads have finished loading the current tile.
    __syncthreads();

    // Compute the dot products for the current tile.
    compute_dot_products(As[current], Bs[current], registers, out_values);

    __syncthreads();

    // Update current value.
    current ^= 1;
}

// Wait for the last shared-memory stores to finish.
__pipeline_wait_prior(0);
__syncthreads();
compute_dot_products(As[current], Bs[current], registers, out_values);

// No change.
run_epilogue(...);
```

Note that I had to scale down the shared-memory usage by reducing $BK$ from 32 to 16, since double buffering requires twice the shared-memory capacity. Keeping `BK = 32` would limit the number of blocks that can be scheduled concurrently, which in turn reduces performance instead.

The updated performance table can be found below.

| Kernel # | Performance (TFLOPs/s) | % cuBLAS performance |
|:---|:---:|:---:|:---:|
| cuBLAS | 31.66 | 100% |
| kernel 01: naive | 2.05 | 6.47% |
| kernel 02: block tiling | 3.49 | 11.02% |
| kernel 03: 2D thread coarsening | 20.05 | 63.33% |
| kernel 04: vectorized memory access | 26.16 | 82.63% |
| kernel 05: warp tiling | 26.19 | 82.72% |
| kernel 06: warp tiling, subdivided | 28.70 | 90.65% |
| kernel 07: transpose `As` | 29.71 | 93.84% |
| kernel 08: asynchronous copy + double buffering üÜï | 30.28 | 95.64% |

We have now reached approximately 96% of cuBLAS performance on single-precision GEMM!

<hr class="hr-top" /><hr />

# 5. Tensor cores

Multiply-add is the most frequently used operation in modern neural networks as it is the building block of fully-connected and convolutional layers (read [here](https://docs.nvidia.com/cutlass/latest/media/docs/cpp/implicit_gemm_convolution.html) if you're curious about how convolutional layers can be mapped into implicit GEMM). As modern neural networks continue to grow in size and demand significantly higher computational capability, the ability to accelerate multiply-add operations becomes crucial.

To address this need, [the Volta GPU architecture introduced tensor cores](https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf), a specialized component of the GPU architecture designed to accelerate *matrix* multiply-accumulate operations. Tensor cores provide significantly higher throughput than CUDA cores. For example, on my RTX 5070 Ti, the peak *BF16 throughput with FP32 accumulation* reaches 87.9 TFLOPs/s, compared to only 43.9 TFLOPs/s of peak non-tensor core throughput.

There are two ways to use tensor cores: via `wmma` or `mma` instructions. Both instruction types include load, store, and computation operations, which are performed collectively by all threads in a warp.

`wmma` instructions can be used through a high-level API or via explicit PTX instructions. Overall, `wmma` is more straightforward to use, but also more rigid, as it does not allow modification of how matrix elements are distributed across threads. `mma`, on the other hand, can only be used via PTX instructions and requires a deeper level of understanding of how it works, but offers greater flexibility. For example, custom swizzling can be used with `mma` but not with `wmma` (to my current knowledge).

<!-- START OF DIV -->
<div class="div-author-note">
<h4>Author's Note</h4>
Notice two specific details in the explanation above: (1) the emphasis on <i>matrix</i> in matrix multiply-accumulate and (2) the particular example of <i>BF16 throughput with FP32 accumulation</i>.
<br /><br />
We will examine these points in more details later, but essentially, tensor cores operate on matrices with predetermined sizes and data types for each matrix, depending on the instruction being used (<code>wmma</code> or <code>mma</code>).
</div>
<!-- END OF DIV -->

<hr class="hr-top" /><hr />

# 6. Half-precision GEMM implementations

Tensor cores do not currently support all data types. The available combinations of data types for the multiplicand and accumulator matrices are listed in [this table](https://docs.nvidia.com/cuda/parallel-thread-execution/#warp-level-matrix-data-types). For instance, 
among floating-point data types, there is currently no support for using FP32 for both the multiplicands and the accumulator. Because of that, we will focus on half-precision GEMM implementations in the rest of the post.

In particular, we will use the 16-bit alternative floating-point data type BFloat16 (`.bf16`) for the multiplicands $A$ and $B$, and the 32-bit IEEE floating-point data type (`.f32`) for the accumulator $C$.

<!-- START OF DIV -->
<div class="div-interested">
<h4>If You're Curious...</h4>
BFloat16 stands for <i>Brain Floating Point Format</i>. It is a custom 16-bit floating-point format developed by Google Brain with the goal of accelerating matrix multiplication operations, where each multiply-accumulate operation uses BFloat16 for the multiplication and 32-bit IEEE floating point (FP32) for accumulator.
<br />

<div style="background-color: white; padding-top: 20px; margin-top: 20px;">
    <img src="/assets/images/2025-11-10-gemm/hgmm_bfloat16.png" alt="The floating-point formats.">
</div>
<p style="text-align: center;"><i>The three floating-point formats; figure taken from <a href="https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus">the Google Cloud's BFloat16 blog</a>.</i></p>

As shown in the figure above, BFloat16 has the same dynamic range as FP32 due to having the same number of exponent bits. This allows simpler to and from FP32 compared to FP16 and provides greater robustness against numerical instability during training. Given its 16-bit size, BFloat16 reduces memory usage and can lead to overall faster performance, making it advantageous for layers that do not require a high level of precision. You can read more about the BFloat16 format <a href="https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus">here</a>.
</div>
<!-- END OF DIV -->

To implement HGEMM, I begin by enabling half-precision computation in *some* of the SGEMM kernels implemented above and examine their performance relative to SGEMM. I then explore the use of tensor cores via the `wmma` API and optimize the implementation from there. Each kernel implementation is compared against cuBLAS performance.

The performance of some of the kernels above on half-precision GEMM is as follows.

| Kernel # | SGEMM Performance (TFLOPs/s) | HGEMM Performance (TFLOPs/s) | % HGEMM cuBLAS performance |
|:---|:---:|:---:|:---:|
| cuBLAS | 31.66 | 88.81 | 100% |
| kernel 01: naive | 2.05 | 2.90 | 3.26% |
| kernel 02: block tiling | 3.49 | 2.80 | 3.15% |
| kernel 04: vectorized memory access | 26.16 | 26.33 | 29.65% |
| kernel 05: warp tiling | 26.19 | 26.81 | 30.19% |
| kernel 06: warp tiling, subdivided | 28.70 | 27.23 | 30.66% |

While cuBLAS performance shoots up to 88.81 TFLOPs/s for HGEMM, kernels up to Kernel 06 show performance similar to SGEMM, so I stopped there and shifted focus to tensor cores. I also did not look into why the performance of Kernel 02 dropped in HGEMM - something to look into in the future.

The kernels using tensor cores are as follows.

1. [**Kernel 09: Tensor cores (wmma API)**](#kernel-09-tensor-cores-wmma-api)
2. [**Kernel 10 & 11: Tensor cores + asynchronous gmem loads + double buffering**](#kernel-10--11-tensor-cores--asynchronous-gmem-loads--double-buffering)
4. [**Kernel 12: Tensor cores + three-level pipeline**](#kernel-12-tensor-cores--three-level-pipeline)
5. [**Kernel 13: Tensor cores (mma instructions)**](#kernel-13-tensor-cores-mma-instructions)


#### Kernel 09: Tensor cores (wmma API)

I use the [warp matrix functions](https://docs.nvidia.com/cuda/archive/12.2.2/cuda-c-programming-guide/index.html#warp-matrix-functions) defined in the namespace `nvcuda::wmma`; I would highly recommend reading the documentation first before continuing reading this post.

To understand how tensor cores and the wmma APIs can be used, it helps to visualize and compare the new kernel to Kernel 06 (warptiling, subdivided):

![Kernel 06 vs Kernel 09](/assets/images/2025-11-10-gemm/kernel9.png)
<p style="text-align: center;"><i>The comparison between Kernel 06 and Kernel 09.</i></p>

There are three main differences between Kernel 06 and Kernel 09 as illustrated above:

1. We no longer need to consider the $TM \times TN$ thread tiling explicitly. Although each thread still computes multiple output elements, wmma now determines which threads load data and compute which output elements.
2. Some changes in terminology:
    - `WSUBN` (or `WN / WNITER`) becomes `WMMA_N`
    - `WSUBM` (or `WM / WMITER`) becomes `WMMA_M`
    
    In other words, rather than deciding the number of subwarp tiles and deriving the subwarp size, we now choose the subwarp size directly, and the number of subwarp tiles follows.
3. Kernel 09 introduces `WMMA_K`, which determines the number of tiles along the $BK$ dimension.

`WMMA_M`, `WMMA_N`, and `WMMA_K` indicate the matrix size; the variety of matrix sizes supported by tensor cores are predetermined and can be found [here](https://docs.nvidia.com/cuda/archive/12.2.2/cuda-c-programming-guide/index.html#element-types-and-matrix-sizes). In the case of using wmma and the BFloat16 data type, the supported matrix sizes are 16x16x16, 32x8x16, and 8x32x16 in the format of `mxnxk`.

<!-- START OF DIV -->
<div class="div-author-note">
<h4>Author's Note</h4>
Since there are only three possible matrix sizes, I compared all of them and ultimately chose 16x16x16 for all kernels as it consistently delivers strong performance across the board.
</div>
<!-- END OF DIV -->

To implement the code, we first create `fragments` for $A$ and $B$ multiplicands and the accumulator.

```c++
// Create fragments.
nvcuda::wmma::fragment<
    nvcuda::wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, __nv_bfloat16, nvcuda::wmma::row_major> a_frag;
nvcuda::wmma::fragment<
    nvcuda::wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, __nv_bfloat16, nvcuda::wmma::row_major> b_frag;
nvcuda::wmma::fragment<
    nvcuda::wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, float> acc_frags[NUM_WMMA_M][NUM_WMMA_N];

// Initialize the accumulator fragments.
for (int wmma_row_idx{0}; wmma_row_idx < NUM_WMMA_M; ++wmma_row_idx)
    for (int wmma_col_idx{0}; wmma_col_idx < NUM_WMMA_N; ++wmma_col_idx)
        nvcuda::wmma::fill_fragment(acc_frags[wmma_row_idx][wmma_col_idx], 0.0f);
```

Tensor cores mainly handle stage 2 (dot-product computation) and 3 (epilogue + output stores), so there are no changes to stage 1 (shared-memory stores). In this case, I reuse `load_gmem_to_smem` function from Kernel 04 (vectorized memory access).

Stage 2 (dot-product computation) implementation is as follows.

```c++
for (uint wmma_k_offset{0u}; wmma_k_offset < BK; wmma_k_offset += WMMA_K) {
    for (int wmma_row_idx{0}; wmma_row_idx < NUM_WMMA_M; ++wmma_row_idx) {
        // Load from As to a_frag.
        nvcuda::wmma::load_matrix_sync(
            a_frag,
            &As[(wmma_row_idx * WMMA_M) * BK + wmma_k_offset],
            BK
        );

        for (int wmma_col_idx{0}; wmma_col_idx < NUM_WMMA_N; ++wmma_col_idx) {
            // Load from Bs to b_frag.
            nvcuda::wmma::load_matrix_sync(
                b_frag,
                &Bs[wmma_k_offset * BN + (wmma_col_idx * WMMA_N)],
                BN
            );

            // Perform matrix multiplication.
            nvcuda::wmma::mma_sync(
                acc_frags[wmma_row_idx][wmma_col_idx],
                a_frag,
                b_frag,
                acc_frags[wmma_row_idx][wmma_col_idx]
            );
        }
    }
}
```

As mentioned earlier, we no longer explicitly specify which elements each thread is responsible for. Instead, all threads in a warp collaboratively load elements from `As` and `Bs`, store them into the fragments `a_frag` and `b_frag`, perform matrix multiplication, and store the results in the corresponding `acc_frags` tile.

Stage 3 (epilogue + output stores) code can be found below.

```c++
// Create a new fragment to store elements from C.
nvcuda::wmma::fragment<
    nvcuda::wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, float> c_frag;

for (int wmma_row_idx{0}; wmma_row_idx < NUM_WMMA_M; ++wmma_row_idx) {
    for (int wmma_col_idx{0}; wmma_col_idx < NUM_WMMA_N; ++wmma_col_idx) {
        uint const C_row_offset{wmma_row_idx * WMMA_M};
        uint const C_col_offset{wmma_col_idx * WMMA_N};

        nvcuda::wmma::load_matrix_sync(
            c_frag,
            &C[C_row_offset * N + C_col_offset],
            N,
            nvcuda::wmma::mem_row_major
        );

        for (int i{0}; i < c_frag.num_elements; ++i) {
            c_frag.x[i] = alpha * acc_frags[wmma_row_idx][wmma_col_idx].x[i] + beta * c_frag.x[i];
        }

        nvcuda::wmma::store_matrix_sync(
            &C[C_row_offset * N + C_col_offset], c_frag, N, nvcuda::wmma::mem_row_major
        );
    }
}
```

Notice that the current wmma APIs do not incorporate $\alpha$ and $\beta$, so these must be handled manually. We also **cannot simply assume that the fragments are stored in a particular order**. For now, the only guarantee is that elements can be mapped safely between accumulator fragments. In other words, elements with the same index in different accumulator fragments correspond to the same position in the original matrix, regardless of where that index is stored internally.

<!-- START OF DIV -->
<div class="div-interested">
<h4>If You're Curious...</h4>
Unlike <code>wmma</code>, <code>mma</code> requires users to explicitly define how the matrix elements are discributed across threads, so the documentation also specifies the layout of each fragment. <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#warp-level-matrix-fragment-mma-16816-float">Here </a> is an example where we can see the layouts of the multiplicand $A$ fragment, the multiplicand $B$ fragment, and the accumulator fragments in the 16x8x16 matrix for <i>each floating-point data types</i>.
</div>
<!-- END OF DIV -->

The performance of the kernel can be found in the table below. As we can see, utilizing tensor cores significantly improves the throughput to around 70% of cuBLAS performance!

| Kernel # | Performance (TFLOPs/s) | % cuBLAS performance |
|:---|:---:|:---:|:---:|
| cuBLAS | 88.81 | 100% |
| kernel 01: naive | 2.90 | 3.26% |
| kernel 02: block tiling | 2.80 | 3.15% |
| kernel 04: vectorized memory access | 26.33 | 29.65% |
| kernel 05: warp tiling | 26.81 | 30.19% |
| kernel 06: warp tiling, subdivided | 27.23 | 30.66% |
| kernel 09: tensor cores (wmma API) üÜï | 65.38 | 73.62% |


<!-- START OF DIV -->
<div class="div-author-note">
<h4>Author's Note</h4>
If you're still unclear about what fragments are and what they mean for each thread, don't worry! I struggled to understand this as well at the beginning; I only managed to grasp the concept after attempting to use the <code>mma</code> instructions.
<br /><br />
We can treat fragments as registers that hold multiple elements. In the <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#warp-level-matrix-fragment-mma-16816-float">16x8x16 matrix</a>, for instance, 16x16 elements are collectively loaded from <code>As</code> to <code>a_frag</code> by a warp. This means that each thread handles $16 \times 16 / 32 = 8$ elements. So <code>a_frag</code> in each thread contains eight elements, and wmma determines which specific elements each thread is responsible for. Similarly, both <code>b_frag</code> and <code>acc_frag</code> hold four elements each in each thread. 
</div>
<!-- END OF DIV -->

#### Kernel 10 & 11: Tensor cores + asynchronous gmem loads + double buffering

In both Kernel 10 and 11, we apply the two optimization techniques from Kernel 08 one by one, specifically:

- **Kernel 10**: we use `__pipeline_memcpy_async` to bypass registers (and the L1 cache depending on the transfer size) when loading data from global memory into shared memory. Don't forget to add `__pipeline_wait_prior(0)` immediately after each global memory load to ensure that all data loads have been completed before resuming with the dot-product computation.
- **Kernel 11**: we overlap computation and shared-memory stores to hide memory latency. To achieve this, we double the shared-memory buffers: one buffer holds the data used by the current computation, while the other holds the data needed for the next computation.

These modifications result in the following performance improvements:

| Kernel # | Performance (TFLOPs/s) | % cuBLAS performance |
|:---|:---:|:---:|:---:|
| cuBLAS | 88.81 | 100% |
| kernel 01: naive | 2.90 | 3.26% |
| kernel 02: block tiling | 2.80 | 3.15% |
| kernel 04: vectorized memory access | 26.33 | 29.65% |
| kernel 05: warp tiling | 26.81 | 30.19% |
| kernel 06: warp tiling, subdivided | 27.23 | 30.66% |
| kernel 09: tensor cores (wmma API) | 65.38 | 73.62% |
| kernel 10: tensor cores + async gmem loads üÜï | 73.77 | 83.06% |
| kernel 11: tensor cores + double buffering üÜï | 80.33 | 90.45% |

#### Kernel 12: Tensor cores + three-level pipeline

In Kernel 08, we define two levels for overlapping: shared-memory stores and dot-product computation. The dot-product computation itself, however, can be further divided into two steps: (1) register stores and (2) the actual dot-product computation. What if we try overlapping all these three steps, as illustrated below?

![Kernel 12 pipeline.](/assets/images/2025-11-10-gemm/kernel12.png)
<p style="text-align: center;"><i>Kernel 12.</i></p>

To do so, we first divide the dot-product computation into the following functions:

**1\. Loading from shared memory to registers**

```c++
for (uint wmma_k_idx{0}; wmma_k_idx < NUM_WMMA_K; ++wmma_k_idx) {
    uint const wmma_k_offset{wmma_k_idx * WMMA_K};

    // Load A elements from smem to registers.
    for (int wmma_row_idx{0}; wmma_row_idx < NUM_WMMA_M; ++wmma_row_idx) {
        nvcuda::wmma::load_matrix_sync(
            a_frags[wmma_row_idx][wmma_k_idx],
            &As[(warp_row_offset + wmma_row_idx * WMMA_M) * BK + wmma_k_offset],
            BK
        );
    }

    // Load B elements from smem to registers.
    for (int wmma_col_idx{0}; wmma_col_idx < NUM_WMMA_N; ++wmma_col_idx) {
        nvcuda::wmma::load_matrix_sync(
            b_frags[wmma_k_idx][wmma_col_idx],
            &Bs[wmma_k_offset * BN + (warp_col_offset + wmma_col_idx * WMMA_N)],
            BN
        );
    }
}
```

**2\. Run the actual dot-product computation**

```c++
for (uint wmma_k_idx{0u}; wmma_k_idx < NUM_WMMA_K; ++wmma_k_idx) {
    for (int wmma_row_idx{0}; wmma_row_idx < NUM_WMMA_M; ++wmma_row_idx) {
        for (int wmma_col_idx{0}; wmma_col_idx < NUM_WMMA_N; ++wmma_col_idx) {
            nvcuda::wmma::mma_sync(
                acc_frags[wmma_row_idx][wmma_col_idx],
                a_frags[wmma_row_idx][wmma_k_idx],
                b_frags[wmma_k_idx][wmma_col_idx],
                acc_frags[wmma_row_idx][wmma_col_idx]
            );
        }
    }
}
```

Since we can no longer directly compute the matrix multiplication, we now need $A$ and $B$ fragments for *each WMMA tile*. 

My first thought of how it overall should be implemented can be illustrated below.

![First thought on the order of the operations in Kernel 12](/assets/images/2025-11-10-gemm/kernel12_order.png)
<p style="text-align: center;"><i>First thought on the order of the operations in Kernel 12.</i></p>

With the implementation pseudocode below.

```c++
load_gmem_to_smem(tile[0], A, B, As[0], Bs[0]);

load_gmem_to_smem(tile[1], A, B, As[1], Bs[1]);

load_smem_to_regs(As[0], Bs[0], A_frags[0], B_frags[0]); // tile[0].

int current{0};

for (tile_index = 2; tile_index < NUM_TILES; ++tile_index) {
    load_gmem_to_smem(tile[tile_index], A, B, As[current], Bs[current]);

    load_smem_to_regs(As[current ^ 1], Bs[current ^ 1], A_frags[current ^ 1], B_frags[current ^ 1]); // tile[1] in 1st iter.

    compute_dot_products(A_frags[current], B_frags[current], acc_frags); // tile [0] in 1st iter.

    current ^= 1;
}

load_smem_to_regs(As[current ^ 1], Bs[current ^ 1], A_frags[current ^ 1], B_frags[current ^ 1]);

compute_dot_products(A_frags[current], B_frags[current], acc_frags);

compute_dot_products(A_frags[current ^ 1], B_frags[current ^ 1], acc_frags);

// No change.
run_epilogue(...);
```

Recall that we already need to have $A$ and $B$ fragments for each WMMA tile. Given the implementation above, we also need two buffers for each multiplicand fragment, just like how we allocate two buffers for the shared memory. Because of this, we end up with significantly more registers per thread, which further reduces both the occupancy and the performance in my case.

Fortunately, we can actually **switch the order of the operations within the loop such that we no longer need the second buffer for the fragments**. The key observation is that it is not necessary to perform the register stores for the *next* computation **before** carrying out the dot-product computation for the *current* one! So within the `for` loop, we can actually do the following order:

```c++
for (tile_index = 2; tile_index < NUM_TILES; ++tile_index) {
    load_gmem_to_smem(tile[tile_index], A, B, As[current], Bs[current]);

    compute_dot_products(A_frags, B_frags, acc_frags); // tile [0] in 1st iter.

    // A_frags and B_frags are now free to use, so we can reuse it in the register stores!
    load_smem_to_regs(As[current ^ 1], Bs[current ^ 1], A_frags, B_frags); // tile[1] in 1st iter.

    current ^= 1;
}
```

Similarly, we run the dot-product computation first after the `for` loop ends.

```c++
compute_dot_products(A_frags, B_frags, acc_frags);

load_smem_to_regs(As[current ^ 1], Bs[current ^ 1], A_frags, B_frags);

compute_dot_products(A_frags, B_frags, acc_frags);
```

With these modifications, we're able to keep register usage sufficiently low and reach 98% of cuBLAS performance!

| Kernel # | Performance (TFLOPs/s) | % cuBLAS performance |
|:---|:---:|:---:|:---:|
| cuBLAS | 88.81 | 100% |
| kernel 01: naive | 2.90 | 3.26% |
| kernel 02: block tiling | 2.80 | 3.15% |
| kernel 04: vectorized memory access | 26.33 | 29.65% |
| kernel 05: warp tiling | 26.81 | 30.19% |
| kernel 06: warp tiling, subdivided | 27.23 | 30.66% |
| kernel 09: tensor cores (wmma API) | 65.38 | 73.62% |
| kernel 10: tensor cores + async gmem loads | 73.77 | 83.06% |
| kernel 11: tensor cores + double buffering | 80.33 | 90.45% |
| kernel 12: tensor cores + three-level pipeline üÜï | 87.13 | 98.11% |

#### Kernel 13: Tensor cores (mma instructions)



# PTX

```
wmma.load.a.sync.aligned.layout.shape{.ss}.atype r, [p] {, stride}
wmma.load.b.sync.aligned.layout.shape{.ss}.btype r, [p] {, stride}
wmma.load.c.sync.aligned.layout.shape{.ss}.ctype r, [p] {, stride}
.layout = {.row, .col};
.shape  = {.m16n16k16, .m8n32k16, .m32n8k16};
.ss     = {.global, .shared{::cta}};
.atype  = {.bf16 };
.btype  = {.bf16 };
.ctype  = {.f32 };
```

- How to replace p (address operand)?
- How to replace r (register)?
- https://forums.developer.nvidia.com/t/n-x-n-register-array-of-16-bit-dattype-how-much-registers-does-it-actually-occupy/34871?utm_source=chatgpt.com (why we need to do reinterpret_cast)
- 64-bit vs 32-bit https://forums.developer.nvidia.com/t/why-do-i-need-to-convert-a-pointer-to-shared-address-space-before-using-the-ldmatrix-instruction/274466/2 

- ldmatrix: Each address corresponds to the start of a matrix row
- swizzling: permute WITHIN row + keeps 8 (or n) elements in each MMA tile row together

# Resources

- [How to Optimize a CUDA Matmul Kernel for cuBLAS-like Performance: a Worklog](https://siboehm.com/articles/22/CUDA-MMM) by Simon Boehm (2022).
- [Understanding the Roofline Model](https://accelerated-computing.academy/fall25/resources/roofline/) taken from MIT 6.S894: Accelerated Computing Resources page
- [CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide)
- PTX ISA
- PTX inline documentation
- Asynchronous Data Copies section, [Advanced Performance Optimization in CUDA](https://www.nvidia.com/en-us/on-demand/session/gtc24-s62192/) by Igor Terentyev (2024).
- https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html#gpu-arch