---
layout: post
title:  "GEMM"
date:   2025-11-10
categories: cuda
katex: true
---

It's widely known that implementing and optimizing GEMM (**GE**neral **M**atrix **M**ultiplication) is a fundamental prerequisite for learning GPU programming. In this post, we'll walk through the process of implementing and optimizing single-precision GEMM--from a brief overview of what the operation does to advanced optimizations using Tensor Cores.

I'll primarily reference [Simon's GEMM post](https://siboehm.com/articles/22/CUDA-MMM) as a foundation, and then explore how to push performance further by leveraging additional features available on my GPU (RTX 5070 Ti). All performance measurements in this posts were taken on an RTX 5070 Ti and the implementation code can be found [here](https://github.com/kathsucurry/cuda_matrix_multiplication).

This post is organized into the following sections:

1. [**What does GEMM do?**](#1-what-does-gemm-do)
2. **Problem setup**
3. **Kernel implementation**
    - Kernel 01: Naive implementation
    - Kernel 02: Block tiling
    - Kernel 03: 2D thread coarsening
    - Kernel 04: Vectorization

# 1. What does GEMM do?

Given three matrices $A$, $B$, and $C$

