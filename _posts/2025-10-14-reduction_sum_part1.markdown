---
layout: post
title:  "Reduction (Sum): part 1"
date:   2025-10-14
categories: cuda
---

*This post is still a WIP.*

As part of the "Reduction (Sum)" series, this post outlines my process and approach to implementing and optimizing sum reduction kernels. I use Mark Harris's [*Optimizing Parallel Reduction in CUDA* deck](https://developer.download.nvidia.com/assets/cuda/files/reduction.pdf) as a reference, with modifications based on the insights I've gained along the way. My approach can be summarized as follows.

1. **Implement the reduction kernel** and ensure that the output is correct using the verification process described in the [previous post]({% link _posts/2025-10-14-reduction_sum_part0.markdown %}).
2. **Profile the kernel** using [NVIDIA Nsight Compute](https://developer.nvidia.com/nsight-compute). I highly recommend you to watch [this kernel profiling lecture](https://www.youtube.com/watch?v=F_BazucyCMw&t=1s) hosted by GPU Mode if you are interested in using Nsight Compute.
3. **Inspect the PTX/SASS**, if necessary, to better understand the performance characteristics or identify optimization opportunities.


# Kernel 0: naive interleaved addressing

The figure below summarizes the kernel implementation.

![image Naive interleaved addressing](/assets/images/2025-10-14-reduction_sum_part1/kernel0_interleaved_address.png)
<p style="text-align: center;"><i>The first interleaved addressing kernel by Mark Harris.</i></p>

And you can find the relevant code below. Note the identical and interchangeable definition of the terms *batch* and *block* in this post.

```c++
template <size_t NUM_THREADS>
__global__ void batched_original_interleaved_address_0(
    float* __restrict__ Y,
    float const* __restrict__ X,
    size_t num_elements_per_batch
) {
    size_t const block_idx{blockIdx.x};
    size_t const thread_idx{threadIdx.x};
    // Allocate the shared memory with length of the number of threads in the block/batch.
    __shared__ float shared_data[NUM_THREADS];

    // Shift the input accordingly to the batch index.
    X += block_idx * num_elements_per_batch;
    // Store a single element per thread in shared memory.
    shared_data[thread_idx] = X[thread_idx];
    __syncthreads();

    for (size_t stride = 1; stride < NUM_THREADS; stride *= 2) {
        if (thread_idx % (2 * stride) == 0)
            shared_data[thread_idx] += shared_data[thread_idx + stride];
        __syncthreads();
    }

    if (thread_idx == 0)
        Y[block_idx] = shared_data[0];
}
```

Recall from the previous post that my GPU (RTX 5070 Ti) peak bandwidth is 896 GB/s and there are `2048 * 1024 * 256` number of elements. Running the kernel 50 times repeatedly with different numbers of threads per block leads to the following performance table. 

| # Threads/block | Runtime (Âµs) | Mean Effective Bandwidth | % Peak Bandwidth |
|:---:|:---:|:---:|:---:|
| 128 | 5,017.23 | 431.366 | 48.14 |
| 256 | 5,098.87 | 423.229 | 47.23 |
| 512 | 5,652.35 | 380.67 | 42.48 |
| 1,024 | 10,193.5 | 210.878 | 23.53 |

We observe that using 128 threads yields the best performance among all the options. Increasing the number of threads to 256 slightly degrades performance; using 512 threads reduces it further, and with 1,024 threads, performance drops significantly--the achieved bandwidth is nearly half that of the 128-thread configuration.

*What explains the performance differences across thread configurations?*

Let's start with the 1,024-thread configuration. In the previous post, we briefly discussed how the resources required by each block can limit the number of blocks that can be scheduled in each multiprocessor. These resources include the number of threads, the number of registers, and the amount of shared memory required per block.

The maximum threads per multiprocessor on my GPU is 1,536 (equivalent to 48 warps), as reported by the `maxThreadsPerMultiProcessor` field from `cudaGetDeviceProperties`. With a block size of 1,024, the GPU can only place 1 block (i.e., 32 warps) per SM. This means that out of the 48 warps slots available on the SM, only 32 are active, resulting in a theoretical occupancy of `32 / 48 * 100% = 66.67%`. In contrast, the other thread configurations are able to achieve 100% theoretical occupancy.

As mentioned earlier, the number of registers and the amount of shared memory per block can also limit occupancy; however, this is not the case for any of the thread configurations used here for this particular kernel.

*What about the remaining thread configurations? Why does the 128-thread configuration perform the best?*

Looking at the profile generated by Nsight Compute, all three thread configurations exhibit thread divergence.

![image Thread Divergence](/assets/images/2025-10-14-reduction_sum_part1/kernel0_thread_divergence.png)
<p style="text-align: center;"><i>One of the suggestions from Nsight Compute for the 128-thread configuration, which is to address the thread divergence issue.</i></p>

In our case, thread divergence occurs because the threads performing the sum operation in the `for` loop (i.e., the active threads whose indices satisfy the `if` condition) are scattered across multiple warps. In the 128-thread configuration, there are 64 active threads during the first iteration, but these threads are not grouped contiguously. The active threads consist of those with indices 0, 2, 4, 6, and so on. As a result, all 4 warps are partially active and must execute the loop body, leading to warp divergence. In ideal scenario, only 2 warps would be fully active and the remaining warps inactive, minimizing divergence. **Larger thread blocks can be more negatively affected by thread divergence since more warps increase the opportunity for divergence**, which may help explain why the runtime worsens as the number of threads per block increases from 128 to 256 and 512.

Note that we will only use the 128-thread configuration for all the subsequent implementations.

# Kernel 1: interleaved addressing with thread divergence resolved

The only difference between Kernel 0 and Kernel 1 lies in how we select the active threads--i.e., the threads that perform the sum operation. The goal is to ensure that the active threads are **contiguous** such that only `ceil(# active threads / 32)` warp(s) are active. The figure below summarizes the implementation.

![image Interleaved addressing with thread divergence resolved](/assets/images/2025-10-14-reduction_sum_part1/kernel1_interleaved_address.png)
<p style="text-align: center;"><i>The interleaved addressing kernel with thread divergence resolved.</i></p>

As for the code, we only need to change the following:

```c++
    for (size_t stride = 1; stride < NUM_THREADS; stride *= 2) {
        if (thread_idx % (2 * stride) == 0)
            shared_data[thread_idx] += shared_data[thread_idx + stride];
        __syncthreads();
    
```

to:

```c++
    for (size_t stride = 1; stride < NUM_THREADS; stride *= 2) {
        size_t index = 2 * stride * thread_idx;

        if (index < NUM_THREADS)
            shared_data[index] += shared_data[index + stride];
        __syncthreads();
    }
```

Below is the updated performance table.

| Kernel # <br />(128 threads/block) | Runtime (Âµs) | Mean Effective Bandwidth | % Peak Bandwidth |
|:---:|:---:|:---:|:---:|
| kernel 0 | 5,017.23 | 431.366 | 48.14 |
| ðŸ†• kernel 1 ðŸ†• | 4,962.33 | 436.138 | 48.67 |

There is a slight improvement from Kernel 0 to Kernel 1, although it's not particularly significant. 

When profiling the kernel with Nsight Compute, we identify a new performance optimization opportunity: addressing *Shared Load Bank Conflicts*.


<TODO: add screenshot here>

*What are shared load (or shared memory) bank conflicts, and why does Kernel 1 have this issue?*

To answer this question, we need to examine how shared memory is organized. According to [CUDA C Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#shared-memory-12-0), for devices with compute capability 12.0 (such as the RTX 5070 Ti), shared memory is divided into **32 banks** or memory modules. **This organization allows simultaneous access to `n` distinct addresses as long as they map to `n` *different* memory banks**. Successive 32-bit words are mapped to successive banks, and each bank provides a bandwidth of 32 bits per clock cycle. A shared memory block containing 128 floating-point elements is organized as illustrated by the figure below.

![image Shared memory banks](/assets/images/2025-10-14-reduction_sum_part1/kernel1_banks.png)
<p style="text-align: center;"><i>How the 128 elements are divided into 32 banks in the shared memory.</i></p>

One or more **bank conflicts** occur when multiple threads in the same warp request *different memory addresses* that are mapped to the same memory bank. These accesses are then serialized, which consequently reduces the effective bandwidth by a factor equal to the number of the memory requests targeting that bank.

In Kernel 0, no bank conflicts occur because threads within the same warp always request addresses that are mapped to different banks. The figure below illustrates the memory requests by threads in warp 0 during the first two iterations of the `for` loop. In iteration 0 (`stride = 1`), thread 0 requests shared memory elements at indices 0 and 1 (located in bank 0 and 1, respectively), thread index 2 requests elements at index 2 and 3 (located in bank 2 and 3), and so on. The same pattern remains consistent across subsequent iterations and for all other warps.

![image Memory requests in Kernel 0](/assets/images/2025-10-14-reduction_sum_part1/kernel1_kernel0_requests.png)
<p style="text-align: center;"><i>The memory requests by warp 0 threads in Kernel 0 during the first two iterations</i></p>

In contrast, since the threads performing memory access are grouped into a contiguous block in Kernel 1, bank conflicts occur consistently in all iterations. The figure below illustrates the first two iterations, where a **2-way bank conflicts** occur in the first iteration and **4-way bank conflicts** occur in the second iteration. This behavior can significantly reduce the overall effective bandwidth.

![image Memory requests in Kernel 1](/assets/images/2025-10-14-reduction_sum_part1/kernel1_kernel1_requests.png)
<p style="text-align: center;"><i>The memory requests by warp 0 threads in Kernel 1 during the first two iterations. Bank conflicts occur in both iterations.</i></p>

> ðŸ“ **Note**
>
> A bank conflict does not occur when multiple threads within the same warp access any address within the same 32-bit word. In the case of floating-point elements (i.e., 32 bits), bank conflicts do not occur when multiple threads access the *same* address. For read operations, the requested element is broadcast to the requesting threads, while for write operations, each address is written by one of the threads. More details can be found in [CUDA C Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#shared-memory-5-x).