---
layout: post
title:  "Reduction (Sum): part 1"
date:   2025-10-14
categories: cuda
---

*This post is still a WIP.*

As part of the "Reduction (Sum)" series, this post outlines my process and approach to implementing and optimizing sum reduction kernels. I use Mark Harris's [*Optimizing Parallel Reduction in CUDA* deck](https://developer.download.nvidia.com/assets/cuda/files/reduction.pdf) as a reference, with modifications based on the insights I've gained along the way. My approach can be summarized as follows.

1. **Implement the reduction kernel** and ensure that the output is correct using the verification process described in the [previous post]({% link _posts/2025-10-14-reduction_sum_part0.markdown %}).
2. **Analyze the kernel's performance** to understand whether (and why) it performs better or worse. Some methods include:
    -  **Profile the kernel** using [NVIDIA Nsight Compute](https://developer.nvidia.com/nsight-compute). I highly recommend you to watch [this kernel profiling lecture](https://www.youtube.com/watch?v=F_BazucyCMw&t=1s) hosted by GPU Mode if you are interested in using Nsight Compute.
    - **Inspect the PTX/SASS**, if necessary, to better understand the performance characteristics or identify optimization opportunities.


# Kernel 0: naive interleaved addressing

The figure below summarizes the kernel implementation.

![image Naive interleaved addressing](/assets/images/2025-10-14-reduction_sum_part1/kernel0_interleaved_address.png)
<p style="text-align: center;"><i>The first interleaved addressing kernel by Mark Harris.</i></p>

And you can find the relevant code below. Note the identical and interchangeable definition of the terms *batch* and *block* in this post.

```c++
template <size_t NUM_THREADS>
__global__ void batched_interleaved_address_naive(
    float* __restrict__ Y,
    float const* __restrict__ X,
    size_t num_elements_per_batch
) {
    size_t const block_idx{blockIdx.x};
    size_t const thread_idx{threadIdx.x};
    // Allocate the shared memory with length of the number of threads in the block/batch.
    __shared__ float shared_data[NUM_THREADS];

    // Shift the input accordingly to the batch index.
    X += block_idx * num_elements_per_batch;
    // Store a single element per thread in shared memory.
    shared_data[thread_idx] = X[thread_idx];
    __syncthreads();

    for (size_t stride = 1; stride < NUM_THREADS; stride *= 2) {
        if (thread_idx % (2 * stride) == 0)
            shared_data[thread_idx] += shared_data[thread_idx + stride];
        __syncthreads();
    }

    if (thread_idx == 0)
        Y[block_idx] = shared_data[0];
}
```

Recall from the previous post that my GPU (RTX 5070 Ti) peak bandwidth is 896 GB/s and there are `2048 * 1024 * 256` number of elements. Running the kernel 50 times repeatedly with different numbers of threads per block leads to the following performance table. 

| # Threads/block | Runtime (µs) | Mean Effective Bandwidth | % Peak Bandwidth |
|:---:|:---:|:---:|:---:|
| 128 | 5,017.23 | 431.366 | 48.14 |
| 256 | 5,098.87 | 423.229 | 47.23 |
| 512 | 5,652.35 | 380.67 | 42.48 |
| 1,024 | 10,193.5 | 210.878 | 23.53 |

We observe that using 128 threads yields the best performance among all the options. Increasing the number of threads to 256 slightly degrades performance; using 512 threads reduces it further, and with 1,024 threads, performance drops significantly--the achieved bandwidth is nearly half that of the 128-thread configuration.

*What explains the performance differences across thread configurations?*

Let's start with the 1,024-thread configuration. In the previous post, we briefly discussed how the resources required by each block can limit the number of blocks that can be scheduled in each multiprocessor. These resources include the number of threads, the number of registers, and the amount of shared memory required per block.

The maximum threads per multiprocessor on my GPU is 1,536 (equivalent to 48 warps), as reported by the `maxThreadsPerMultiProcessor` field from `cudaGetDeviceProperties`. With a block size of 1,024, the GPU can only place 1 block (i.e., 32 warps) per SM. This means that out of the 48 warps slots available on the SM, only 32 are active, resulting in a theoretical occupancy of `32 / 48 * 100% = 66.67%`. In contrast, the other thread configurations are able to achieve 100% theoretical occupancy.

As mentioned earlier, the number of registers and the amount of shared memory per block can also limit occupancy; however, this is not the case for any of the thread configurations used here for this particular kernel.

*What about the remaining thread configurations? Why does the 128-thread configuration perform the best?*

Looking at the profile generated by Nsight Compute, all three thread configurations exhibit thread divergence.

![image Thread Divergence](/assets/images/2025-10-14-reduction_sum_part1/kernel0_thread_divergence.png)
<p style="text-align: center;"><i>One of the suggestions from Nsight Compute for the 128-thread configuration, which is to address the thread divergence issue.</i></p>

In our case, thread divergence occurs because the threads performing the sum operation in the `for` loop (i.e., the active threads whose indices satisfy the `if` condition) are scattered across multiple warps. In the 128-thread configuration, there are 64 active threads during the first iteration, but these threads are not grouped contiguously. The active threads consist of those with indices 0, 2, 4, 6, and so on. As a result, all 4 warps are partially active and must execute the loop body, leading to warp divergence. In ideal scenario, only 2 warps would be fully active and the remaining warps inactive, minimizing divergence. **Larger thread blocks can be more negatively affected by thread divergence since more warps increase the opportunity for divergence**, which may help explain why the runtime worsens as the number of threads per block increases from 128 to 256 and 512.

Note that we will only use the 128-thread configuration for all the subsequent implementations.

# Kernel 1: interleaved addressing with thread divergence resolved

The only difference between Kernel 0 and Kernel 1 lies in how we select the active threads--i.e., the threads that perform the sum operation. The goal is to ensure that the active threads are **contiguous** such that only `ceil(# active threads / 32)` warp(s) are active. The figure below summarizes the implementation.

![image Interleaved addressing with thread divergence resolved](/assets/images/2025-10-14-reduction_sum_part1/kernel1_interleaved_address.png)
<p style="text-align: center;"><i>The interleaved addressing kernel with thread divergence resolved.</i></p>

As for the code, we only need to change the following:

```c++
    for (size_t stride = 1; stride < NUM_THREADS; stride *= 2) {
        if (thread_idx % (2 * stride) == 0)
            shared_data[thread_idx] += shared_data[thread_idx + stride];
        __syncthreads();
    
```

to:

```c++
    for (size_t stride = 1; stride < NUM_THREADS; stride *= 2) {
        size_t index = 2 * stride * thread_idx;

        if (index < NUM_THREADS)
            shared_data[index] += shared_data[index + stride];
        __syncthreads();
    }
```

Below is the updated performance table.

| Kernel # <br />(128 threads/block) | Runtime (µs) | Mean Effective Bandwidth | % Peak Bandwidth |
|:---:|:---:|:---:|:---:|
| kernel 0 | 5,017.23 | 431.366 | 48.14 |
| 🆕 kernel 1 🆕 | 4,962.33 | 436.138 | 48.67 |

There is a slight improvement from Kernel 0 to Kernel 1, although it's not particularly significant. 

When profiling the kernel with Nsight Compute, we identify a new performance optimization opportunity: addressing *Shared Load Bank Conflicts*.


![image Bank conflicts](/assets/images/2025-10-14-reduction_sum_part1/kernel1_bank_conflict.png)
<p style="text-align: center;"><i>A new optimization opportunity observed on Nsight Compute: addressing bank conflicts.</i></p>

*What are shared load (or shared memory) bank conflicts, and why does Kernel 1 have this issue?*

To answer this question, we need to examine how shared memory is organized. According to [CUDA C Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#shared-memory-12-0), for devices with compute capability 12.0 (such as the RTX 5070 Ti), shared memory is divided into **32 banks** or memory modules. **This organization allows simultaneous access to `n` distinct addresses as long as they map to `n` *different* memory banks**. Successive 32-bit words are mapped to successive banks, and each bank provides a bandwidth of 32 bits per clock cycle. A shared memory block containing 128 floating-point elements is organized as illustrated by the figure below.

![image Shared memory banks](/assets/images/2025-10-14-reduction_sum_part1/kernel1_banks.png)
<p style="text-align: center;"><i>How the 128 elements are divided into 32 banks in the shared memory.</i></p>

One or more **bank conflicts** occur when multiple threads in the same warp request *different memory addresses* that are mapped to the same memory bank. These accesses are then serialized, which consequently reduces the effective bandwidth by a factor equal to the number of the memory requests targeting that bank.

In Kernel 0, no bank conflicts occur because threads within the same warp always request addresses that are mapped to different banks. The figure below illustrates the read memory requests by threads in warp 0 (thread index 0 to 31) during the first two iterations of the `for` loop. In iteration 0 (`stride = 1`), thread 0 requests shared memory elements at indices 0 and 1 (located in bank 0 and 1, respectively), thread index 2 requests elements at index 2 and 3 (located in bank 2 and 3), and so on. The same pattern remains consistent across subsequent iterations and for all other warps.

![image Memory requests in Kernel 0](/assets/images/2025-10-14-reduction_sum_part1/kernel1_kernel0_requests.png)
<p style="text-align: center;"><i>The memory requests by warp 0 threads in Kernel 0 during the first two iterations</i></p>

In contrast, since the threads performing memory access are grouped into a contiguous block in Kernel 1, bank conflicts occur consistently in all iterations. The figure below illustrates the read memory accesses during the first two iterations, where a **2-way bank conflicts** occur in the first iteration and **4-way bank conflicts** occur in the second iteration. This behavior can significantly reduce the overall effective bandwidth.

![image Memory requests in Kernel 1](/assets/images/2025-10-14-reduction_sum_part1/kernel1_kernel1_requests.png)
<p style="text-align: center;"><i>The memory requests by warp 0 threads in Kernel 1 during the first two iterations. Bank conflicts occur in both iterations.</i></p>

In summary, although Kernel 1 resolves the thread divergence issue present in Kernel 0, the shared memory bank conflict issue still needs to be addressed.

> 📝 **Note**
>
> A bank conflict does not occur when multiple threads within the same warp access any address within the same 32-bit word. In the case of 4-byte floating-point elements (i.e., 32 bits), bank conflicts do not occur when multiple threads access the *same* address. For read operations, the requested element is broadcast to the requesting threads, while for write operations, each address is written by one of the threads. More details can be found in [CUDA C Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#shared-memory-5-x).


# Kernel 2: sequential addressing

The next implementation addresses the shared memory bank conflict issue observed in Kernel 1 by storing the intermediate sum values in **sequential shared memory addresses**. The following figure illustrates the updated approach.

![image Sequential addressing](/assets/images/2025-10-14-reduction_sum_part1/kernel2_sequential_address.png)
<p style="text-align: center;"><i>The sequential addressing kernel.</i></p>

In the code, we simply change the following rows from Kernel 1:

```c++
    for (size_t stride = 1; stride < NUM_THREADS; stride *= 2) {
        size_t index = 2 * stride * thread_idx;

        if (index < NUM_THREADS)
            shared_data[index] += shared_data[index + stride];
        __syncthreads();
    }
```

to:

```c++
    for (size_t stride = NUM_THREADS / 2; stride > 0; stride >>= 1) {
        if (thread_idx < stride)
            shared_data[thread_idx] += shared_data[thread_idx + stride];
        __syncthreads();
    }
```

Interestingly, **we don't observe any speedup between Kernel 1 and Kernel 2**, which contrasts with nearly 2x performance improvement reported in Mark Harris's presentation.

| Kernel # <br />(128 threads/block) | Runtime (µs) | Mean Effective Bandwidth | % Peak Bandwidth |
|:---:|:---:|:---:|:---:|
| kernel 0 | 5,017.23 | 431.366 | 48.14 |
| kernel 1 | 4,962.33 | 436.138 | 48.67 |
| 🆕 kernel 2 🆕 | 4,968.31 | 435.613 | 48.61 |

*What accounts for the discrepancy between the performance differences observed in this post and those reported in Mark Harris's presentation?*

One possible explanation is **the improved shared memory performance under bank conflict conditions in newer GPU generations**. While I haven't been able to find any explicit statement from NVIDIA confirming this, a [2016 paper titled *"Dissecting GPU Memory Hierarchy Through Microbenchmarking"*](https://ieeexplore.ieee.org/document/7445236) demonstrated that newer GPU architectures at the time exhibited a lower performance penalty under bank conflict scenarios (see Table 8 in the paper). It's worth noting that Mark Harris's presentation dates back to 2007. It is possible that the penalty is even smaller on more recent architectures, such as Blackwell. That said, addressing shared memory bank conflicts remains important for achieving optimal performance.

# Kernel 3: repositioning `__syncthreads()`

Kernel 2 includes two thread synchronizations: (1) after loading elements from global memory to shared memory, and 2) at the end of each `for` loop iteration to ensure all thread updates to shared memory are completed. We can further simplify this by **removing one synchronization and repositioning the other to the beginning of the `for` loop**.

By synchronizing threads at the **start** of the loop, we ensure that the initial shared memory load from global memory is complete before any computation begins. The only synchronization removed is the **final one**, which would have occurred at the end of the **last** iteration: when only a single thread updates the first shared memory element.

This removal is safe because the same thread that performs the final update is also responsible for writing the result back to global memory after the loop ends. Since no other threads are involved at this point, **no synchronization is necessary**.

To summarize, we change the following code:

```c++
    // Store a single element per thread in shared memory.
    shared_data[thread_idx] = X[thread_idx];
    __syncthreads(); // The first synchronization.

    for (size_t stride = NUM_THREADS / 2; stride > 0; stride >>= 1) {
        if (thread_idx < stride)
            shared_data[thread_idx] += shared_data[thread_idx + stride];
        __syncthreads(); // The second synchronization.
    }
```

to

```c++
    // Store a single element per thread in shared memory.
    shared_data[thread_idx] = X[thread_idx];

    for (size_t stride = NUM_THREADS / 2; stride > 0; stride >>= 1) {
        __syncthreads(); // The repositioned synchronization.
        if (thread_idx < stride)
            shared_data[thread_idx] += shared_data[thread_idx + stride];
    }
```

which gives us a better performance as shown below.

| Kernel # <br />(128 threads/block) | Runtime (µs) | Mean Effective Bandwidth | % Peak Bandwidth |
|:---:|:---:|:---:|:---:|
| kernel 0 | 5,017.23 | 431.366 | 48.14 |
| kernel 1 | 4,962.33 | 436.138 | 48.67 |
| kernel 2 | 4,968.31 | 435.613 | 48.61 |
| 🆕 kernel 3 🆕 | 4,512.13 | 479.654 | 53.53 |


# Kernel 4: thread coarsening

Recall in all the kernels so far, after loading elements from global memory into shared memory, half of the threads (`N / 2`) become idle at the start of the `for` loop. In the second iteration, `N / 4` threads become idle, then `N / 8` in the third, and so on, until only one thread remains active in the final iteration. This progressive underutilization of resources, exacerbated by the attempt to maximize parallelism (i.e., launching as many threads as possible), can significantly impact performance.

One fundamental optimization technique to address this issue is **thread coarsening**. The idea is to have each thread perform more work, thereby reducing the total number of threads launched and minimizing parallelization overhead. In this context, I'm merging Mark Harris's Reduction #4 (First Add During Load) and Reduction #7 (Multiple Adds Per Thread), and will experiment with varying the number of elements each thread loads and adds. The implementation is illustrated by the figure below.

![image Thread coarsening](/assets/images/2025-10-14-reduction_sum_part1/kernel4_thread_coarsening.png)
<p style="text-align: center;"><i>The thread coarsening kernel where the number of elements per thread is set to 3.</i></p>

In the implementation code, we replace the code line that stores an element from global memory to shared memory:

```c++
    // Store a single element per thread in shared memory.
    shared_data[thread_idx] = X[thread_idx];
```

with the following `for` loop:

```c++
    // Compute the number of elements each thread will process.
    size_t const num_elements_per_thread{(num_elements_per_batch + NUM_THREADS - 1) / NUM_THREADS};
    
    // Initialize the sum variable.
    float sum{0.0f};
    
    for (size_t i = 0; i < num_elements_per_thread; ++i) {
        size_t const offset{thread_idx + i * NUM_THREADS};
        if (offset < num_elements_per_batch)
            sum += X[offset];
    }
    shared_data[thread_idx] = sum;
```

Experimenting with varying number of elements per thread using the 128-thread configuration leads to the following result.

| # elements per thread | Runtime (µs) | Mean Effective Bandwidth | % Peak Bandwidth |
|:---:|:---:|:---:|:---:|
| 1 | 4,643.64 | 466.070 | 52.01 |
| 2 | 2,553.32 | 844.339 | 94.23 |
| 4 | 2,532.10 | 849.761 | 94.83 |
| 8 | 2,523.05 | 851.978 | 95.08 |
| 16 | 2,519.19 | 852.866 | 95.18 |
| 32 | 2,517.57 | 853.206 | 95.22 |
| 64 | 2,517.31 | 853.191 | 95.21 |
| 128 | 2,518.28 | 852.810 | 95.17 |
| 256 | 2,517.61 | 853.011 | 95.19 |
| 512 | 2,519.89 | 852.225 | 95.11 |
| 1,024 | 2,524.26 | 850.743 | 94.94 |
| 2,048 | 2,526.57 | 849.964 | 94.85 |

> 📝 **Note**
>
> For the case of adding two elements per thread, running the simple operation `shared_data[thread_idx] = X[thread_idx] + X[thread_idx + NUM_THREADS];`--following the approach of Mark Harris's Kernel 4--instead of using a `for` loop yields effective bandwidth of 848.272 GB/s, which is slightly higher than the value shown in the table (844.339 GB/s). This improvement is likely due to the simplicity of the operation compared to the `for` loop implementation, which involves additional complexity such as using an extra register to store intermediate sums and evaluating an `if` condition within each iteration.

Based on the table above, we observe a significant performance jump: from 52% in the one-element-per-thread configuration to 94% in the two-elements-per-thread configuration! Performance continues to gradually improve as the number of elements per thread increases, peaking when each thread processes 32 or 64 elements.

Additionally, the Nsight Compute profiles show that achieved occupancy increases from 86% in Kernel 2 to 99% in Kernel 4. (Recall that both kernels have a theoretical occupancy of 100%) This improvement is likely due to the significantly lower overhead associated with launching fewer threads in Kernel 4.

*Why are the elements being summed in each thread separated by a stride that is a multiple of the block size?*

Notice that when computing `offset` in the `for` loop, each thread sums elements at indices of the form `thread_idx + <multiple of NUM_THREADS>`. You might wonder why we don't simply sum `n` contiguous elements per thread (where `n` is the number of elements assigned to each thread). As discussed in [Part 0](/_posts/2025-10-14-reduction_sum_part0.markdown), global memory access is relatively slow, so optimizing memory access efficiency is critical.

By introducing a stride (i.e., a gap between the elements each thread accesses), we ensure that **all threads in a warp collectively access contiguous memory locations in each iteration** (see the figure below). This pattern enables the hardware to potentially combine these accesses into a single memory transaction--a process known as **memory coalescing**, which significantly improves memory throughput. 

![image Memory coalescing](/assets/images/2025-10-14-reduction_sum_part1/kernel4_memory_coalesce.png)
<p style="text-align: center;"><i>A coalesced access pattern in which contiguous memory locations are accessed during each `for` loop iteration.</i></p>

> 💬 **Optional reading**
>
> Using an uncoalesced memory access pattern with 32 elements per thread results in an effective memory bandwidth of 776.987 GB/s, compared to 853.208 GB/s with coalesced access.

To summarize, we have the updated performance table below.


| Kernel # <br />(128 threads/block) | Runtime (µs) | Mean Effective Bandwidth | % Peak Bandwidth |
|:---:|:---:|:---:|:---:|
| kernel 0 | 5,017.23 | 431.366 | 48.14 |
| kernel 1 | 4,962.33 | 436.138 | 48.67 |
| kernel 2 | 4,968.31 | 435.613 | 48.61 |
| kernel 3 | 4,512.13 | 479.654 | 53.53 |
| 🆕 kernel 4 🆕 | 2,517.57 | 853.208 | 95.22 |

At this point, we have reached 95% peak bandwidth. Let's see if the rest of the kernels in Mark Harris's presentation can improve it even further.

# Kernel 5: loop unrolling

Loop unrolling is an optimization technique in which programmers or compilers rewrite a loop as a sequence of repeated, independent statements. The goal is to reduce or eliminate the overhead associated with loop control operations, such as index incrementation and end-of-loop condition checks. You can find more details [here](https://en.wikipedia.org/wiki/Loop_unrolling).

Mark Harris's presentation provides two kernels that perform loop unrolling: Reduction #5 (Unroll The Last Warp) and Reduction #6 (Competely Unrolled).

> ⚠️ **Caution** ⚠️
>
> The changes added to Reduction #5 is now obsolete on newer GPUs, which I will discuss further later.


**Reduction #5 (Kernel 5 v1): unroll the last warp**

In this implementation, only the final active warp (i.e., threads with indices < 32) is unrolled by defining and calling a helper function `warp_reduce` where these threads perform the summation without requiring any explicit thread synchronization. This is possible because all threads within a warp execute in **lockstep**, meaning, all active threads follow the same instruction stream simultanouesly, and none can advance ahead or fall behind. Note, however, that this strict lockstep behavior applies primarily to old GPU architectures; we'll discuss the implications of this later when addressing potential issues.

We add 3 modifications to the code:

1\. Define the helper function `warp_reduce()`.

```c++
__device__ void warp_reduce(volatile float* shared_data, size_t thread_idx) {
    shared_data[thread_idx] += shared_data[thread_idx + 32];
    shared_data[thread_idx] += shared_data[thread_idx + 16];
    shared_data[thread_idx] += shared_data[thread_idx + 8];
    shared_data[thread_idx] += shared_data[thread_idx + 4];
    shared_data[thread_idx] += shared_data[thread_idx + 2];
    shared_data[thread_idx] += shared_data[thread_idx + 1];
}
```

2\. Modify the `for` loop condition.

```c++
    // Replace `stride > 0` with `stride > NUM_THREADS_PER_WARP`.
    for (size_t stride = NUM_THREADS / 2; stride > NUM_THREADS_PER_WARP; stride >>= 1) {
        ...
    }
```

3\. Call `warp_reduce()` when thread index is less than 32.

```c++
    if (thread_idx < NUM_THREADS_PER_WARP) {
        __syncthreads();
        warp_reduce(shared_data, thread_idx);
    }
```

The `volatile` qualitifer is applied to the `shared_data` variable in `warp_reduce()` to prevent the compiler from optimizing away or caching its values. Since multiple threads may update the same shared memory locations, marking the variable as `volatile` ensures that each thread always reads the most up-to-date value written by other threads, preventing reordering or register caching of shared memory accesses.

**Reduction #6 (Kernel 5 v2): completely unrolled**

The maximum number of threads per block is fixed to 1,024 for current GPUs, which makes it possible to completely unroll the `for` loop since it relies on the number of threads per block. We simply add these modifications to the implementation code:

1\. Pass the block size constant using template and add `if` condition for each sum operation in `warp_reduce()`.

```c++
template <size_t NUM_THREADS>
__device__ void warp_reduce(volatile float* shared_data, size_t thread_idx) {
    if (NUM_THREADS >= 64) shared_data[thread_idx] += shared_data[thread_idx + 32];
    if (NUM_THREADS >= 32) shared_data[thread_idx] += shared_data[thread_idx + 16];
    if (NUM_THREADS >= 16) shared_data[thread_idx] += shared_data[thread_idx +  8];
    if (NUM_THREADS >=  8) shared_data[thread_idx] += shared_data[thread_idx +  4];
    if (NUM_THREADS >=  4) shared_data[thread_idx] += shared_data[thread_idx +  2];
    if (NUM_THREADS >=  2) shared_data[thread_idx] += shared_data[thread_idx +  1];
}
```

2\. Completely unroll the summation `for` loop by replacing the follwing:

```c++
    for (size_t stride = NUM_THREADS / 2; stride > NUM_THREADS_PER_WARP; stride >>= 1) {
        __syncthreads();
        if (thread_idx < stride)
            shared_data[thread_idx] += shared_data[thread_idx + stride];
    }

    if (thread_idx < NUM_THREADS_PER_WARP) {
        __syncthreads();
        warp_reduce(shared_data, thread_idx);
    }
```

with:

```c++
    if (NUM_THREADS == 1024){
        if (thread_idx < 512) shared_data[thread_idx] += shared_data[thread_idx + 512];
        __syncthreads();
    }
    if (NUM_THREADS >= 512){
        if (thread_idx < 256) shared_data[thread_idx] += shared_data[thread_idx + 256];
        __syncthreads();
    }
    if (NUM_THREADS >= 256){
        if (thread_idx < 128) shared_data[thread_idx] += shared_data[thread_idx + 128];
        __syncthreads();
    }
    if (NUM_THREADS >= 128){
        if (thread_idx < 64) shared_data[thread_idx] += shared_data[thread_idx + 64];
        __syncthreads();
    }

    if (thread_idx < NUM_THREADS_PER_WARP)
        warp_reduce<NUM_THREADS>(shared_data, thread_idx);
```

The performance of the two kernels can be found below.

| Kernel # <br />(128 threads/block) | Runtime (µs) | Mean Effective Bandwidth | % Peak Bandwidth |
|:---:|:---:|:---:|:---:|
| kernel 4 | 2,517.57 | 853.206 | 95.22 | 
| kernel 5: unroll last warp | 2,517.51 | 853.226 | 95.22 |
| kernel 5: completely unrolled | 2,517.56 | 853.212 | 95.22 |

There's only a very slight effective bandwidth improvement from Kernel 4 to Kernel 5, and no additional gain when moving from unrolling only the last warp to fully unrolling the summation `for` loop.

*Why is the performance improvement from Kernel 4 to Kernel 5 so minimal?*

To answer this question, I copied the implementation code of Kernel 4 and both versions of Kernel 5 into [Godbolt](https://godbolt.org/z/cPqhMPPhn).

Focusing first on Kernel 4, we see from the PTX code that the `for` loop has already been **automatically unrolled** by the compiler when inspecting the PTX code (see the figure below). This happens because modern CUDA compilers are capable of automatically unrolling loops--either completely or partially--whenever doing so is expected to improve performance.

![image Kernel 4 already unrolled](/assets/images/2025-10-14-reduction_sum_part1/kernel5_thread_coarsening_unrolled.png)
<p style="text-align: center;"><i>The compiler has already automatically unrolled the `for` loop in Kernel 4. The PTX code lines highlighted in blue correspond to line 33 in the source code.</i></p>

To disable this automatic unrolling, we can add `#pragma unroll 1` right before the `for` loop and see the differences in the PTX code.

![image Kernel 4 with disabled unrolling](/assets/images/2025-10-14-reduction_sum_part1/kernel5_thread_coarsening.png)
<p style="text-align: center;"><i>Adding `#pragma unroll 1` disables the automatic unrolling. The PTX code lines highlighted in blue correspond to line 34 in the source code.</i></p>

Given this behavior, we can also expect no additional performance gain when manually completely unrolling the `for` loop in the second version of Kernel 5.

There is, however, one noticeable difference between Kernel 4 and Kernel 5 version 1 (which unrolls only the last warp). When we reveal the linked code for the lines under `warp_reduce()` in Kernel 5 version 1, we can see that the following PTX instructions:

```
ld.volatile.shared.f32 	%f28, [%r1];      // Load a register variable %f28 from shared memory with address %r1.
ld.volatile.shared.f32 	%f29, [%r1+128];  // Load a register variable %f29 from shared memory with address $r1+128.
add.f32                 %f30, %f29, %f28; // Add %f29 and %f28, then store the result in %f30.
st.volatile.shared.f32 	[%r1], %f30;      // Store %f30 in shared memory with address %r1.
```

are repeated contiguously (with different memory addresses and registers), without any of the overhead associated with thread synchronization or conditional checks that are present in Kernel 4's PTX code.

![image Comparing PTX code between Kernel 4 and Kernel 5 (unroll last warp)](/assets/images/2025-10-14-reduction_sum_part1/kernel5_ptx.png)
<p style="text-align: center;"><i>The main difference between Kernel 4 and Kernel 5 version 1.</i></p>

This may explain the slight effective bandwidth gain between Kernel 4 and Kernel 5.



